{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688306f4-58d3-4279-b9a2-e89fb33c94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "# Read the file in chunks\n",
    "def process_chunk(chunk, unique_building_blocks, unique_molecules):\n",
    "    # Update the unique building blocks set\n",
    "    unique_building_blocks.update(chunk['buildingblock1_smiles'].unique())\n",
    "    unique_building_blocks.update(chunk['buildingblock2_smiles'].unique())\n",
    "    unique_building_blocks.update(chunk['buildingblock3_smiles'].unique())\n",
    "    \n",
    "    # Update the unique molecules set\n",
    "    unique_molecules.update(chunk['molecule_smiles'].unique())\n",
    "\n",
    "# Load the parquet file\n",
    "file_path = './train.parquet'\n",
    "batch_size = 100000\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "# Initialize sets to keep track of unique building blocks and molecules\n",
    "unique_building_blocks = set()\n",
    "unique_molecules = set()\n",
    "# Set pandas display option to avoid truncation of long strings\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Iterate over the parquet file in batches\n",
    "num_row_groups = parquet_file.num_row_groups\n",
    "\n",
    "# Initialize variables to store the total number of rows and rows with binds = 1\n",
    "total_rows = 0\n",
    "binds_1_count = 0\n",
    "\n",
    "for i in range(num_row_groups):\n",
    "    # Read a batch of rows\n",
    "    row_group = parquet_file.read_row_group(i).to_pandas()\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"First few rows from the first row group:\")\n",
    "        print(row_group.head())\n",
    "\n",
    "        # Print the molecule smiles column\n",
    "        print(f\"Row group {i} molecule smiles:\")\n",
    "        print(row_group['molecule_smiles'])\n",
    "\n",
    "        # Update the total number of rows and the count of binds = 1\n",
    "    total_rows += len(row_group)\n",
    "    binds_1_count += row_group['binds'].sum()\n",
    "    \n",
    "    # Process the current chunk\n",
    "    process_chunk(row_group, unique_building_blocks, unique_molecules)\n",
    "\n",
    "# Calculate the percentage of rows with binds = 1 for the entire dataset\n",
    "percentage_binds_1 = (binds_1_count / total_rows) * 100\n",
    "\n",
    "# Print the percentage of rows with binds = 1 for the entire dataset\n",
    "print(f\"Percentage of rows with binds = 1 in the entire dataset: {percentage_binds_1:.2f}%\")\n",
    "\n",
    "# Output the total unique counts\n",
    "print(f\"Total number of unique building blocks: {len(unique_building_blocks)}\")\n",
    "print(f\"Total number of unique molecules: {len(unique_molecules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b9d17-28e6-4d61-a618-389e55565cdb",
   "metadata": {},
   "source": [
    "Given that only 0.54% of the data actually, contain bindings, filter the training data down to only select molecule_smiles that bind to at least one protein but not all 3. The idea is to use a model with contrastive loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c767c-7c60-44be-8f3f-af10db4ff634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "# Read the file in chunks\n",
    "def process_chunk(chunk, unique_building_blocks, unique_molecules):\n",
    "    # Update the unique building blocks set\n",
    "    unique_building_blocks.update(chunk['buildingblock1_smiles'].unique())\n",
    "    unique_building_blocks.update(chunk['buildingblock2_smiles'].unique())\n",
    "    unique_building_blocks.update(chunk['buildingblock3_smiles'].unique())\n",
    "    \n",
    "    # Update the unique molecules set\n",
    "    unique_molecules.update(chunk['molecule_smiles'].unique())\n",
    "\n",
    "# Load the parquet file\n",
    "file_path = './train.parquet'\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "# Set pandas display option to avoid truncation of long strings\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Iterate over the parquet file in batches\n",
    "num_row_groups = parquet_file.num_row_groups\n",
    "\n",
    "# Initialize variables to store the total number of rows and rows with binds = 1\n",
    "total_rows = 0\n",
    "binds_1_count = 0\n",
    "\n",
    "# Initialize a dictionary to keep track of binding counts for each molecule\n",
    "molecule_binding_counts = {}\n",
    "\n",
    "# First pass: Identify molecules that bind with at least one protein and track binding counts\n",
    "for i in range(num_row_groups):\n",
    "    # Read a batch of rows\n",
    "    row_group = parquet_file.read_row_group(i).to_pandas()\n",
    "    \n",
    "    # Identify molecules with binds = 1 and count their bindings\n",
    "    binding_molecules = row_group[row_group['binds'] == 1]['molecule_smiles'].value_counts()\n",
    "    for molecule, count in binding_molecules.items():\n",
    "        if molecule not in molecule_binding_counts:\n",
    "            molecule_binding_counts[molecule] = count\n",
    "        else:\n",
    "            molecule_binding_counts[molecule] += count\n",
    "\n",
    "# Filter molecules that bind to at least one but not all three proteins\n",
    "filtered_molecules = {molecule for molecule, count in molecule_binding_counts.items() if 1 <= count < 3}\n",
    "\n",
    "# Second pass: Filter rows with molecules that meet the criteria\n",
    "filtered_data = []\n",
    "\n",
    "for i in range(num_row_groups):\n",
    "    # Read a batch of rows\n",
    "    row_group = parquet_file.read_row_group(i).to_pandas()\n",
    "    \n",
    "    # Filter rows where molecule_smiles is in filtered_molecules\n",
    "    filtered_chunk = row_group[row_group['molecule_smiles'].isin(filtered_molecules)]\n",
    "    filtered_data.append(filtered_chunk)\n",
    "    \n",
    "    # Update the total number of rows and the count of binds = 1 for the filtered data\n",
    "    total_rows += len(filtered_chunk)\n",
    "    binds_1_count += filtered_chunk['binds'].sum()\n",
    "\n",
    "# Concatenate all filtered data\n",
    "filtered_data = pd.concat(filtered_data, ignore_index=True)\n",
    "\n",
    "# Calculate the percentage of rows with binds = 1 for the filtered data\n",
    "percentage_binds_1 = (binds_1_count / total_rows) * 100\n",
    "\n",
    "# Print the percentage of rows with binds = 1 for the filtered data\n",
    "print(f\"Percentage of rows with binds = 1 in the filtered dataset: {percentage_binds_1:.2f}%\")\n",
    "\n",
    "# Output the total unique counts\n",
    "print(f\"Total number of rows in the dataset: {len(filtered_data)}\")\n",
    "\n",
    "# Save the filtered data\n",
    "filtered_data.to_parquet('filtered_train.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc03d5c8-8737-478e-95f5-ca6fd767facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 3623319 rows\n",
      "Test data: 679371 rows\n",
      "Validation data: 226461 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the filtered data\n",
    "filtered_data = pd.read_parquet('filtered_train.parquet')\n",
    "\n",
    "# Get unique molecule_smiles\n",
    "unique_molecule_smiles = filtered_data['molecule_smiles'].unique()\n",
    "\n",
    "# Shuffle the unique molecule_smiles\n",
    "np.random.shuffle(unique_molecule_smiles)\n",
    "\n",
    "# Calculate the number of molecules for each split\n",
    "num_molecules = len(unique_molecule_smiles)\n",
    "train_size = int(0.8 * num_molecules)\n",
    "test_size = int(0.15 * num_molecules)\n",
    "val_size = num_molecules - train_size - test_size\n",
    "\n",
    "# Split the unique molecule_smiles\n",
    "train_smiles = unique_molecule_smiles[:train_size]\n",
    "test_smiles = unique_molecule_smiles[train_size:train_size + test_size]\n",
    "val_smiles = unique_molecule_smiles[train_size + test_size:]\n",
    "\n",
    "# Filter the original data based on the splits\n",
    "train_data = filtered_data[filtered_data['molecule_smiles'].isin(train_smiles)]\n",
    "test_data = filtered_data[filtered_data['molecule_smiles'].isin(test_smiles)]\n",
    "val_data = filtered_data[filtered_data['molecule_smiles'].isin(val_smiles)]\n",
    "\n",
    "# Save the splits to separate Parquet files\n",
    "train_data.to_parquet('train_data_temp.parquet')\n",
    "test_data.to_parquet('test_data_temp.parquet')\n",
    "val_data.to_parquet('val_data_temp.parquet')\n",
    "\n",
    "print(f\"Train data: {len(train_data)} rows\")\n",
    "print(f\"Test data: {len(test_data)} rows\")\n",
    "print(f\"Validation data: {len(val_data)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8070c-400a-47ef-89b6-111251105773",
   "metadata": {},
   "source": [
    "Update the above code to make it easier to create both positive and negative graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaad7eed-71fa-40da-94c8-01fb052aabe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 2415546 rows\n",
      "Test data: 452914 rows\n",
      "Validation data: 150974 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the filtered data\n",
    "filtered_data = pd.read_parquet('filtered_train.parquet')\n",
    "\n",
    "# Initialize a list to store transformed rows\n",
    "transformed_rows = []\n",
    "\n",
    "# Iterate through each group\n",
    "for molecule_smiles, group in filtered_data.groupby('molecule_smiles'):\n",
    "    binding_rows = group[group['binds'] == 1]\n",
    "    non_binding_rows = group[group['binds'] == 0]\n",
    "\n",
    "    if len(binding_rows) == 1 and len(non_binding_rows) == 2:\n",
    "        # Duplicate the single binding row for each non-binding protein\n",
    "        for _, non_bind_row in non_binding_rows.iterrows():\n",
    "            bind_row = binding_rows.iloc[0].copy()\n",
    "            new_row = {\n",
    "                'molecule_smiles': bind_row['molecule_smiles'],\n",
    "                'buildingblock1_smiles': bind_row['buildingblock1_smiles'],\n",
    "                'buildingblock2_smiles': bind_row['buildingblock2_smiles'],\n",
    "                'buildingblock3_smiles': bind_row['buildingblock3_smiles'],\n",
    "                'binds': bind_row['protein_name'],\n",
    "                'not_binds': non_bind_row['protein_name']\n",
    "            }\n",
    "            transformed_rows.append(new_row)\n",
    "    elif len(binding_rows) == 2 and len(non_binding_rows) == 1:\n",
    "        # Duplicate the single non-binding row for each binding protein\n",
    "        for _, bind_row in binding_rows.iterrows():\n",
    "            non_bind_row = non_binding_rows.iloc[0].copy()\n",
    "            new_row = {\n",
    "                'molecule_smiles': non_bind_row['molecule_smiles'],\n",
    "                'buildingblock1_smiles': non_bind_row['buildingblock1_smiles'],\n",
    "                'buildingblock2_smiles': non_bind_row['buildingblock2_smiles'],\n",
    "                'buildingblock3_smiles': non_bind_row['buildingblock3_smiles'],\n",
    "                'binds': non_bind_row['protein_name'],\n",
    "                'not_binds': bind_row['protein_name']\n",
    "            }\n",
    "            transformed_rows.append(new_row)\n",
    "    else:\n",
    "        # If the group does not meet the above criteria, keep it unchanged\n",
    "        for _, row in group.iterrows():\n",
    "            new_row = {\n",
    "                'molecule_smiles': row['molecule_smiles'],\n",
    "                'buildingblock1_smiles': row['buildingblock1_smiles'],\n",
    "                'buildingblock2_smiles': row['buildingblock2_smiles'],\n",
    "                'buildingblock3_smiles': row['buildingblock3_smiles'],\n",
    "                'binds': row['protein_name'],\n",
    "                'not_binds': None\n",
    "            }\n",
    "            transformed_rows.append(new_row)\n",
    "\n",
    "# Create a DataFrame from the transformed rows\n",
    "transformed_data = pd.DataFrame(transformed_rows)\n",
    "\n",
    "# Get unique molecule_smiles\n",
    "unique_molecule_smiles = transformed_data['molecule_smiles'].unique()\n",
    "\n",
    "# Shuffle the unique molecule_smiles\n",
    "np.random.shuffle(unique_molecule_smiles)\n",
    "\n",
    "# Calculate the number of molecules for each split\n",
    "num_molecules = len(unique_molecule_smiles)\n",
    "train_size = int(0.8 * num_molecules)\n",
    "test_size = int(0.15 * num_molecules)\n",
    "val_size = num_molecules - train_size - test_size\n",
    "\n",
    "# Split the unique molecule_smiles\n",
    "train_smiles = unique_molecule_smiles[:train_size]\n",
    "test_smiles = unique_molecule_smiles[train_size:train_size + test_size]\n",
    "val_smiles = unique_molecule_smiles[train_size + test_size:]\n",
    "\n",
    "# Filter the transformed data based on the splits\n",
    "train_data = transformed_data[transformed_data['molecule_smiles'].isin(train_smiles)]\n",
    "test_data = transformed_data[transformed_data['molecule_smiles'].isin(test_smiles)]\n",
    "val_data = transformed_data[transformed_data['molecule_smiles'].isin(val_smiles)]\n",
    "\n",
    "# Save the splits to separate Parquet files\n",
    "train_data.to_parquet('train_data.parquet')\n",
    "test_data.to_parquet('test_data.parquet')\n",
    "val_data.to_parquet('val_data.parquet')\n",
    "\n",
    "print(f\"Train data: {len(train_data)} rows\")\n",
    "print(f\"Test data: {len(test_data)} rows\")\n",
    "print(f\"Validation data: {len(val_data)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5e1a0-a712-46d2-b8b0-a6ab1c96962e",
   "metadata": {},
   "source": [
    "Do some verification of the generated data in the above files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadc47dc-8250-42df-8458-139c444e57d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying train data:\n",
      "Sampled molecule_smiles from train_data.parquet:\n",
      "['O=C(N[Dy])C1CCC(CNc2nc(NCCOc3ccc(F)c(F)c3)nc(NCC(F)(F)C(F)(F)F)n2)CC1'\n",
      " 'CCSCCNc1nc(NCC2CCC(C(=O)N[Dy])CC2)nc(Nc2c(F)cccc2F)n1'\n",
      " 'Cn1cncc1C(CNc1nc(NCC=Cc2cccnc2)nc(Nc2cc(F)c(Br)cc2C(=O)N[Dy])n1)N1CCCC1'\n",
      " 'CC(C)(CCC#N)CNc1nc(NCCC2CCOC2)nc(N[C@@H](CC(=O)N[Dy])Cc2ccc(Br)cc2)n1'\n",
      " 'CCOC(=O)c1cnc(Nc2nc(NCC3CCCCC(F)(F)C3)nc(NC(CC(C)C)C(=O)N[Dy])n2)cn1'\n",
      " 'O=C(N[Dy])C1CCC(CNc2nc(Nc3ccc(F)c([N+](=O)[O-])c3)nc(Nc3nc4ccccc4o3)n2)CC1'\n",
      " 'CN(c1nc(NCc2ccc[n+]([O-])c2)nc(NCC(C)(C)CCC#N)n1)[C@@H](CC1CCCCC1)C(=O)N[Dy]'\n",
      " 'O=C1CCC(Nc2nc(NCC3CCC(C(=O)N[Dy])CC3)nc(Nc3ccc(-c4ncc[nH]4)cc3)n2)CC1'\n",
      " 'O=C(N[Dy])c1ccc(Nc2nc(NCc3ccc(CN4CCCC4=O)cc3)nc(NCC3(Cc4ccccc4)CC3)n2)cc1'\n",
      " 'CNC(=O)c1cc(Oc2ccc(Nc3nc(NCc4nc5c(s4)CCC5)nc(N[C@@H](CC(=O)N[Dy])c4cccc(Cl)c4Cl)n3)cc2)ccn1']\n",
      "\n",
      "Rows from train_data.parquet:\n",
      "                                                                                     molecule_smiles  \\\n",
      "199660                         CC(C)(CCC#N)CNc1nc(NCCC2CCOC2)nc(N[C@@H](CC(=O)N[Dy])Cc2ccc(Br)cc2)n1   \n",
      "199661                         CC(C)(CCC#N)CNc1nc(NCCC2CCOC2)nc(N[C@@H](CC(=O)N[Dy])Cc2ccc(Br)cc2)n1   \n",
      "531130                          CCOC(=O)c1cnc(Nc2nc(NCC3CCCCC(F)(F)C3)nc(NC(CC(C)C)C(=O)N[Dy])n2)cn1   \n",
      "531131                          CCOC(=O)c1cnc(Nc2nc(NCC3CCCCC(F)(F)C3)nc(NC(CC(C)C)C(=O)N[Dy])n2)cn1   \n",
      "687682                                         CCSCCNc1nc(NCC2CCC(C(=O)N[Dy])CC2)nc(Nc2c(F)cccc2F)n1   \n",
      "687683                                         CCSCCNc1nc(NCC2CCC(C(=O)N[Dy])CC2)nc(Nc2c(F)cccc2F)n1   \n",
      "757906                  CN(c1nc(NCc2ccc[n+]([O-])c2)nc(NCC(C)(C)CCC#N)n1)[C@@H](CC1CCCCC1)C(=O)N[Dy]   \n",
      "757907                  CN(c1nc(NCc2ccc[n+]([O-])c2)nc(NCC(C)(C)CCC#N)n1)[C@@H](CC1CCCCC1)C(=O)N[Dy]   \n",
      "796284   CNC(=O)c1cc(Oc2ccc(Nc3nc(NCc4nc5c(s4)CCC5)nc(N[C@@H](CC(=O)N[Dy])c4cccc(Cl)c4Cl)n3)cc2)ccn1   \n",
      "796285   CNC(=O)c1cc(Oc2ccc(Nc3nc(NCc4nc5c(s4)CCC5)nc(N[C@@H](CC(=O)N[Dy])c4cccc(Cl)c4Cl)n3)cc2)ccn1   \n",
      "2184580                      Cn1cncc1C(CNc1nc(NCC=Cc2cccnc2)nc(Nc2cc(F)c(Br)cc2C(=O)N[Dy])n1)N1CCCC1   \n",
      "2184581                      Cn1cncc1C(CNc1nc(NCC=Cc2cccnc2)nc(Nc2cc(F)c(Br)cc2C(=O)N[Dy])n1)N1CCCC1   \n",
      "2511894                        O=C(N[Dy])C1CCC(CNc2nc(NCCOc3ccc(F)c(F)c3)nc(NCC(F)(F)C(F)(F)F)n2)CC1   \n",
      "2511895                        O=C(N[Dy])C1CCC(CNc2nc(NCCOc3ccc(F)c(F)c3)nc(NCC(F)(F)C(F)(F)F)n2)CC1   \n",
      "2579120                   O=C(N[Dy])C1CCC(CNc2nc(Nc3ccc(F)c([N+](=O)[O-])c3)nc(Nc3nc4ccccc4o3)n2)CC1   \n",
      "2579121                   O=C(N[Dy])C1CCC(CNc2nc(Nc3ccc(F)c([N+](=O)[O-])c3)nc(Nc3nc4ccccc4o3)n2)CC1   \n",
      "2871342                    O=C(N[Dy])c1ccc(Nc2nc(NCc3ccc(CN4CCCC4=O)cc3)nc(NCC3(Cc4ccccc4)CC3)n2)cc1   \n",
      "2871343                    O=C(N[Dy])c1ccc(Nc2nc(NCc3ccc(CN4CCCC4=O)cc3)nc(NCC3(Cc4ccccc4)CC3)n2)cc1   \n",
      "2957310                        O=C1CCC(Nc2nc(NCC3CCC(C(=O)N[Dy])CC3)nc(Nc3ccc(-c4ncc[nH]4)cc3)n2)CC1   \n",
      "2957311                        O=C1CCC(Nc2nc(NCC3CCC(C(=O)N[Dy])CC3)nc(Nc3ccc(-c4ncc[nH]4)cc3)n2)CC1   \n",
      "\n",
      "        binds not_binds  \n",
      "199660    sEH      BRD4  \n",
      "199661    sEH       HSA  \n",
      "531130    sEH      BRD4  \n",
      "531131    sEH       HSA  \n",
      "687682    sEH      BRD4  \n",
      "687683    sEH       HSA  \n",
      "757906    sEH      BRD4  \n",
      "757907    sEH       HSA  \n",
      "796284   BRD4       HSA  \n",
      "796285   BRD4       sEH  \n",
      "2184580   HSA      BRD4  \n",
      "2184581   HSA       sEH  \n",
      "2511894   sEH      BRD4  \n",
      "2511895   sEH       HSA  \n",
      "2579120   sEH      BRD4  \n",
      "2579121   sEH       HSA  \n",
      "2871342  BRD4       HSA  \n",
      "2871343  BRD4       sEH  \n",
      "2957310   sEH      BRD4  \n",
      "2957311   sEH       HSA  \n",
      "\n",
      "Corresponding rows from the original file:\n",
      "                                                                     molecule_smiles  \\\n",
      "198303          CCOC(=O)c1cnc(Nc2nc(NCC3CCCCC(F)(F)C3)nc(NC(CC(C)C)C(=O)N[Dy])n2)cn1   \n",
      "198304          CCOC(=O)c1cnc(Nc2nc(NCC3CCCCC(F)(F)C3)nc(NC(CC(C)C)C(=O)N[Dy])n2)cn1   \n",
      "198305          CCOC(=O)c1cnc(Nc2nc(NCC3CCCCC(F)(F)C3)nc(NC(CC(C)C)C(=O)N[Dy])n2)cn1   \n",
      "449661  CN(c1nc(NCc2ccc[n+]([O-])c2)nc(NCC(C)(C)CCC#N)n1)[C@@H](CC1CCCCC1)C(=O)N[Dy]   \n",
      "449662  CN(c1nc(NCc2ccc[n+]([O-])c2)nc(NCC(C)(C)CCC#N)n1)[C@@H](CC1CCCCC1)C(=O)N[Dy]   \n",
      "449663  CN(c1nc(NCc2ccc[n+]([O-])c2)nc(NCC(C)(C)CCC#N)n1)[C@@H](CC1CCCCC1)C(=O)N[Dy]   \n",
      "\n",
      "       protein_name  binds  \n",
      "198303         BRD4      0  \n",
      "198304          HSA      0  \n",
      "198305          sEH      1  \n",
      "449661         BRD4      0  \n",
      "449662          HSA      0  \n",
      "449663          sEH      1  \n",
      "                                                                   molecule_smiles  \\\n",
      "183845                       CCSCCNc1nc(NCC2CCC(C(=O)N[Dy])CC2)nc(Nc2c(F)cccc2F)n1   \n",
      "183846                       CCSCCNc1nc(NCC2CCC(C(=O)N[Dy])CC2)nc(Nc2c(F)cccc2F)n1   \n",
      "183847                       CCSCCNc1nc(NCC2CCC(C(=O)N[Dy])CC2)nc(Nc2c(F)cccc2F)n1   \n",
      "465017       O=C(N[Dy])C1CCC(CNc2nc(NCCOc3ccc(F)c(F)c3)nc(NCC(F)(F)C(F)(F)F)n2)CC1   \n",
      "465018       O=C(N[Dy])C1CCC(CNc2nc(NCCOc3ccc(F)c(F)c3)nc(NCC(F)(F)C(F)(F)F)n2)CC1   \n",
      "465019       O=C(N[Dy])C1CCC(CNc2nc(NCCOc3ccc(F)c(F)c3)nc(NCC(F)(F)C(F)(F)F)n2)CC1   \n",
      "727403       O=C1CCC(Nc2nc(NCC3CCC(C(=O)N[Dy])CC3)nc(Nc3ccc(-c4ncc[nH]4)cc3)n2)CC1   \n",
      "727404       O=C1CCC(Nc2nc(NCC3CCC(C(=O)N[Dy])CC3)nc(Nc3ccc(-c4ncc[nH]4)cc3)n2)CC1   \n",
      "727405       O=C1CCC(Nc2nc(NCC3CCC(C(=O)N[Dy])CC3)nc(Nc3ccc(-c4ncc[nH]4)cc3)n2)CC1   \n",
      "744242  O=C(N[Dy])C1CCC(CNc2nc(Nc3ccc(F)c([N+](=O)[O-])c3)nc(Nc3nc4ccccc4o3)n2)CC1   \n",
      "744243  O=C(N[Dy])C1CCC(CNc2nc(Nc3ccc(F)c([N+](=O)[O-])c3)nc(Nc3nc4ccccc4o3)n2)CC1   \n",
      "744244  O=C(N[Dy])C1CCC(CNc2nc(Nc3ccc(F)c([N+](=O)[O-])c3)nc(Nc3nc4ccccc4o3)n2)CC1   \n",
      "\n",
      "       protein_name  binds  \n",
      "183845         BRD4      0  \n",
      "183846          HSA      0  \n",
      "183847          sEH      1  \n",
      "465017         BRD4      0  \n",
      "465018          HSA      0  \n",
      "465019          sEH      1  \n",
      "727403         BRD4      0  \n",
      "727404          HSA      0  \n",
      "727405          sEH      1  \n",
      "744242         BRD4      0  \n",
      "744243          HSA      0  \n",
      "744244          sEH      1  \n",
      "                                                                  molecule_smiles  \\\n",
      "83685     Cn1cncc1C(CNc1nc(NCC=Cc2cccnc2)nc(Nc2cc(F)c(Br)cc2C(=O)N[Dy])n1)N1CCCC1   \n",
      "83686     Cn1cncc1C(CNc1nc(NCC=Cc2cccnc2)nc(Nc2cc(F)c(Br)cc2C(=O)N[Dy])n1)N1CCCC1   \n",
      "83687     Cn1cncc1C(CNc1nc(NCC=Cc2cccnc2)nc(Nc2cc(F)c(Br)cc2C(=O)N[Dy])n1)N1CCCC1   \n",
      "250137  O=C(N[Dy])c1ccc(Nc2nc(NCc3ccc(CN4CCCC4=O)cc3)nc(NCC3(Cc4ccccc4)CC3)n2)cc1   \n",
      "250138  O=C(N[Dy])c1ccc(Nc2nc(NCc3ccc(CN4CCCC4=O)cc3)nc(NCC3(Cc4ccccc4)CC3)n2)cc1   \n",
      "250139  O=C(N[Dy])c1ccc(Nc2nc(NCc3ccc(CN4CCCC4=O)cc3)nc(NCC3(Cc4ccccc4)CC3)n2)cc1   \n",
      "758769      CC(C)(CCC#N)CNc1nc(NCCC2CCOC2)nc(N[C@@H](CC(=O)N[Dy])Cc2ccc(Br)cc2)n1   \n",
      "758770      CC(C)(CCC#N)CNc1nc(NCCC2CCOC2)nc(N[C@@H](CC(=O)N[Dy])Cc2ccc(Br)cc2)n1   \n",
      "758771      CC(C)(CCC#N)CNc1nc(NCCC2CCOC2)nc(N[C@@H](CC(=O)N[Dy])Cc2ccc(Br)cc2)n1   \n",
      "\n",
      "       protein_name  binds  \n",
      "83685          BRD4      0  \n",
      "83686           HSA      1  \n",
      "83687           sEH      0  \n",
      "250137         BRD4      1  \n",
      "250138          HSA      0  \n",
      "250139          sEH      0  \n",
      "758769         BRD4      0  \n",
      "758770          HSA      0  \n",
      "758771          sEH      1  \n",
      "                                                                                    molecule_smiles  \\\n",
      "152045  CNC(=O)c1cc(Oc2ccc(Nc3nc(NCc4nc5c(s4)CCC5)nc(N[C@@H](CC(=O)N[Dy])c4cccc(Cl)c4Cl)n3)cc2)ccn1   \n",
      "152046  CNC(=O)c1cc(Oc2ccc(Nc3nc(NCc4nc5c(s4)CCC5)nc(N[C@@H](CC(=O)N[Dy])c4cccc(Cl)c4Cl)n3)cc2)ccn1   \n",
      "152047  CNC(=O)c1cc(Oc2ccc(Nc3nc(NCc4nc5c(s4)CCC5)nc(N[C@@H](CC(=O)N[Dy])c4cccc(Cl)c4Cl)n3)cc2)ccn1   \n",
      "\n",
      "       protein_name  binds  \n",
      "152045         BRD4      1  \n",
      "152046          HSA      0  \n",
      "152047          sEH      0  \n",
      "\n",
      "Verifying test data:\n",
      "Sampled molecule_smiles from test_data.parquet:\n",
      "['CC1CCCC(CNc2nc(NC[C@@H]3CCO[C@H]3c3ccnn3C)nc(NC(Cc3ccccc3)C(=O)N[Dy])n2)O1'\n",
      " 'Cn1cc(Nc2nc(NCCC3CC(C)(C)NC3=O)nc(Nc3ccc(C(=O)N[Dy])c(C(=O)O)c3)n2)ccc1=O'\n",
      " 'N#Cc1cccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H]3CC[C@@H](C(=O)N[Dy])C3)n2)n1'\n",
      " 'COC(=O)c1cc(Nc2nc(NCc3cocn3)nc(NC(C(=O)N[Dy])C(C)OC(C)(C)C)n2)ccc1F'\n",
      " 'Cn1c(=O)cc(Nc2nc(NCc3cccc(C(F)(F)F)c3)nc(Nc3c(F)cc(Br)cc3C(=O)N[Dy])n2)[nH]c1=O'\n",
      " 'COc1c(F)ccc(F)c1CNc1nc(NCc2cncc(F)c2)nc(Nc2c(C)cc(Cl)cc2C(=O)N[Dy])n1'\n",
      " 'COC(=O)c1c[nH]nc1Nc1nc(NCc2ccc(O)c3ncccc23)nc(Nc2cc(F)c(F)cc2C(=O)N[Dy])n1'\n",
      " 'COc1cc2nc(Cl)nc(Nc3nc(NCC4CCC(C(=O)N[Dy])CC4)nc(NC4=C(C#N)CCC4)n3)c2cc1OC'\n",
      " 'O=C1OCc2cc(Nc3nc(NCc4cc(C(F)(F)F)co4)nc(NC(CC4CCCCC4)C(=O)N[Dy])n3)ccc21'\n",
      " 'Cc1cc2cc(CNc3nc(NCCn4cnc5ccsc5c4=O)nc(N[C@@H](C(=O)N[Dy])C4CCCCC4)n3)ccc2[nH]1']\n",
      "\n",
      "Rows from test_data.parquet:\n",
      "                                                                           molecule_smiles  \\\n",
      "385702          CC1CCCC(CNc2nc(NC[C@@H]3CCO[C@H]3c3ccnn3C)nc(NC(Cc3ccccc3)C(=O)N[Dy])n2)O1   \n",
      "385703          CC1CCCC(CNc2nc(NC[C@@H]3CCO[C@H]3c3ccnn3C)nc(NC(Cc3ccccc3)C(=O)N[Dy])n2)O1   \n",
      "839962          COC(=O)c1c[nH]nc1Nc1nc(NCc2ccc(O)c3ncccc23)nc(Nc2cc(F)c(F)cc2C(=O)N[Dy])n1   \n",
      "839963          COC(=O)c1c[nH]nc1Nc1nc(NCc2ccc(O)c3ncccc23)nc(Nc2cc(F)c(F)cc2C(=O)N[Dy])n1   \n",
      "884320                 COC(=O)c1cc(Nc2nc(NCc3cocn3)nc(NC(C(=O)N[Dy])C(C)OC(C)(C)C)n2)ccc1F   \n",
      "884321                 COC(=O)c1cc(Nc2nc(NCc3cocn3)nc(NC(C(=O)N[Dy])C(C)OC(C)(C)C)n2)ccc1F   \n",
      "1125860              COc1c(F)ccc(F)c1CNc1nc(NCc2cncc(F)c2)nc(Nc2c(C)cc(Cl)cc2C(=O)N[Dy])n1   \n",
      "1125861              COc1c(F)ccc(F)c1CNc1nc(NCc2cncc(F)c2)nc(Nc2c(C)cc(Cl)cc2C(=O)N[Dy])n1   \n",
      "1200422          COc1cc2nc(Cl)nc(Nc3nc(NCC4CCC(C(=O)N[Dy])CC4)nc(NC4=C(C#N)CCC4)n3)c2cc1OC   \n",
      "1200423          COc1cc2nc(Cl)nc(Nc3nc(NCC4CCC(C(=O)N[Dy])CC4)nc(NC4=C(C#N)CCC4)n3)c2cc1OC   \n",
      "1758722     Cc1cc2cc(CNc3nc(NCCn4cnc5ccsc5c4=O)nc(N[C@@H](C(=O)N[Dy])C4CCCCC4)n3)ccc2[nH]1   \n",
      "1758723     Cc1cc2cc(CNc3nc(NCCn4cnc5ccsc5c4=O)nc(N[C@@H](C(=O)N[Dy])C4CCCCC4)n3)ccc2[nH]1   \n",
      "2088458    Cn1c(=O)cc(Nc2nc(NCc3cccc(C(F)(F)F)c3)nc(Nc3c(F)cc(Br)cc3C(=O)N[Dy])n2)[nH]c1=O   \n",
      "2088459    Cn1c(=O)cc(Nc2nc(NCc3cccc(C(F)(F)F)c3)nc(Nc3c(F)cc(Br)cc3C(=O)N[Dy])n2)[nH]c1=O   \n",
      "2127494          Cn1cc(Nc2nc(NCCC3CC(C)(C)NC3=O)nc(Nc3ccc(C(=O)N[Dy])c(C(=O)O)c3)n2)ccc1=O   \n",
      "2127495          Cn1cc(Nc2nc(NCCC3CC(C)(C)NC3=O)nc(Nc3ccc(C(=O)N[Dy])c(C(=O)O)c3)n2)ccc1=O   \n",
      "2271418  N#Cc1cccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H]3CC[C@@H](C(=O)N[Dy])C3)n2)n1   \n",
      "2271419  N#Cc1cccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H]3CC[C@@H](C(=O)N[Dy])C3)n2)n1   \n",
      "3002732           O=C1OCc2cc(Nc3nc(NCc4cc(C(F)(F)F)co4)nc(NC(CC4CCCCC4)C(=O)N[Dy])n3)ccc21   \n",
      "3002733           O=C1OCc2cc(Nc3nc(NCc4cc(C(F)(F)F)co4)nc(NC(CC4CCCCC4)C(=O)N[Dy])n3)ccc21   \n",
      "\n",
      "        binds not_binds  \n",
      "385702   BRD4       HSA  \n",
      "385703   BRD4       sEH  \n",
      "839962   BRD4       HSA  \n",
      "839963   BRD4       sEH  \n",
      "884320   BRD4       HSA  \n",
      "884321   BRD4       sEH  \n",
      "1125860  BRD4       HSA  \n",
      "1125861  BRD4       sEH  \n",
      "1200422   sEH      BRD4  \n",
      "1200423   sEH       HSA  \n",
      "1758722   HSA      BRD4  \n",
      "1758723   HSA       sEH  \n",
      "2088458   HSA      BRD4  \n",
      "2088459   HSA       sEH  \n",
      "2127494  BRD4       HSA  \n",
      "2127495  BRD4       sEH  \n",
      "2271418   sEH      BRD4  \n",
      "2271419   sEH       HSA  \n",
      "3002732  BRD4       HSA  \n",
      "3002733  BRD4       sEH  \n",
      "\n",
      "Corresponding rows from the original file:\n",
      "                                                                    molecule_smiles  \\\n",
      "307962          COC(=O)c1cc(Nc2nc(NCc3cocn3)nc(NC(C(=O)N[Dy])C(C)OC(C)(C)C)n2)ccc1F   \n",
      "307963          COC(=O)c1cc(Nc2nc(NCc3cocn3)nc(NC(C(=O)N[Dy])C(C)OC(C)(C)C)n2)ccc1F   \n",
      "307964          COC(=O)c1cc(Nc2nc(NCc3cocn3)nc(NC(C(=O)N[Dy])C(C)OC(C)(C)C)n2)ccc1F   \n",
      "715344        COc1c(F)ccc(F)c1CNc1nc(NCc2cncc(F)c2)nc(Nc2c(C)cc(Cl)cc2C(=O)N[Dy])n1   \n",
      "715345        COc1c(F)ccc(F)c1CNc1nc(NCc2cncc(F)c2)nc(Nc2c(C)cc(Cl)cc2C(=O)N[Dy])n1   \n",
      "715346        COc1c(F)ccc(F)c1CNc1nc(NCc2cncc(F)c2)nc(Nc2c(C)cc(Cl)cc2C(=O)N[Dy])n1   \n",
      "966513     O=C1OCc2cc(Nc3nc(NCc4cc(C(F)(F)F)co4)nc(NC(CC4CCCCC4)C(=O)N[Dy])n3)ccc21   \n",
      "966514     O=C1OCc2cc(Nc3nc(NCc4cc(C(F)(F)F)co4)nc(NC(CC4CCCCC4)C(=O)N[Dy])n3)ccc21   \n",
      "966515     O=C1OCc2cc(Nc3nc(NCc4cc(C(F)(F)F)co4)nc(NC(CC4CCCCC4)C(=O)N[Dy])n3)ccc21   \n",
      "1024503  CC1CCCC(CNc2nc(NC[C@@H]3CCO[C@H]3c3ccnn3C)nc(NC(Cc3ccccc3)C(=O)N[Dy])n2)O1   \n",
      "1024504  CC1CCCC(CNc2nc(NC[C@@H]3CCO[C@H]3c3ccnn3C)nc(NC(Cc3ccccc3)C(=O)N[Dy])n2)O1   \n",
      "1024505  CC1CCCC(CNc2nc(NC[C@@H]3CCO[C@H]3c3ccnn3C)nc(NC(Cc3ccccc3)C(=O)N[Dy])n2)O1   \n",
      "\n",
      "        protein_name  binds  \n",
      "307962          BRD4      1  \n",
      "307963           HSA      0  \n",
      "307964           sEH      0  \n",
      "715344          BRD4      0  \n",
      "715345           HSA      1  \n",
      "715346           sEH      1  \n",
      "966513          BRD4      1  \n",
      "966514           HSA      0  \n",
      "966515           sEH      0  \n",
      "1024503         BRD4      1  \n",
      "1024504          HSA      0  \n",
      "1024505          sEH      0  \n",
      "                                                                  molecule_smiles  \\\n",
      "580760  COc1cc2nc(Cl)nc(Nc3nc(NCC4CCC(C(=O)N[Dy])CC4)nc(NC4=C(C#N)CCC4)n3)c2cc1OC   \n",
      "580761  COc1cc2nc(Cl)nc(Nc3nc(NCC4CCC(C(=O)N[Dy])CC4)nc(NC4=C(C#N)CCC4)n3)c2cc1OC   \n",
      "580762  COc1cc2nc(Cl)nc(Nc3nc(NCC4CCC(C(=O)N[Dy])CC4)nc(NC4=C(C#N)CCC4)n3)c2cc1OC   \n",
      "\n",
      "       protein_name  binds  \n",
      "580760         BRD4      0  \n",
      "580761          HSA      0  \n",
      "580762          sEH      1  \n",
      "                                                                          molecule_smiles  \\\n",
      "329284     Cc1cc2cc(CNc3nc(NCCn4cnc5ccsc5c4=O)nc(N[C@@H](C(=O)N[Dy])C4CCCCC4)n3)ccc2[nH]1   \n",
      "329285     Cc1cc2cc(CNc3nc(NCCn4cnc5ccsc5c4=O)nc(N[C@@H](C(=O)N[Dy])C4CCCCC4)n3)ccc2[nH]1   \n",
      "329286     Cc1cc2cc(CNc3nc(NCCn4cnc5ccsc5c4=O)nc(N[C@@H](C(=O)N[Dy])C4CCCCC4)n3)ccc2[nH]1   \n",
      "875782  N#Cc1cccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H]3CC[C@@H](C(=O)N[Dy])C3)n2)n1   \n",
      "875783  N#Cc1cccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H]3CC[C@@H](C(=O)N[Dy])C3)n2)n1   \n",
      "875784  N#Cc1cccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H]3CC[C@@H](C(=O)N[Dy])C3)n2)n1   \n",
      "943972    Cn1c(=O)cc(Nc2nc(NCc3cccc(C(F)(F)F)c3)nc(Nc3c(F)cc(Br)cc3C(=O)N[Dy])n2)[nH]c1=O   \n",
      "943973    Cn1c(=O)cc(Nc2nc(NCc3cccc(C(F)(F)F)c3)nc(Nc3c(F)cc(Br)cc3C(=O)N[Dy])n2)[nH]c1=O   \n",
      "943974    Cn1c(=O)cc(Nc2nc(NCc3cccc(C(F)(F)F)c3)nc(Nc3c(F)cc(Br)cc3C(=O)N[Dy])n2)[nH]c1=O   \n",
      "\n",
      "       protein_name  binds  \n",
      "329284         BRD4      0  \n",
      "329285          HSA      1  \n",
      "329286          sEH      0  \n",
      "875782         BRD4      0  \n",
      "875783          HSA      0  \n",
      "875784          sEH      1  \n",
      "943972         BRD4      0  \n",
      "943973          HSA      1  \n",
      "943974          sEH      0  \n",
      "                                                                   molecule_smiles  \\\n",
      "89394   COC(=O)c1c[nH]nc1Nc1nc(NCc2ccc(O)c3ncccc23)nc(Nc2cc(F)c(F)cc2C(=O)N[Dy])n1   \n",
      "89395   COC(=O)c1c[nH]nc1Nc1nc(NCc2ccc(O)c3ncccc23)nc(Nc2cc(F)c(F)cc2C(=O)N[Dy])n1   \n",
      "89396   COC(=O)c1c[nH]nc1Nc1nc(NCc2ccc(O)c3ncccc23)nc(Nc2cc(F)c(F)cc2C(=O)N[Dy])n1   \n",
      "158619   Cn1cc(Nc2nc(NCCC3CC(C)(C)NC3=O)nc(Nc3ccc(C(=O)N[Dy])c(C(=O)O)c3)n2)ccc1=O   \n",
      "158620   Cn1cc(Nc2nc(NCCC3CC(C)(C)NC3=O)nc(Nc3ccc(C(=O)N[Dy])c(C(=O)O)c3)n2)ccc1=O   \n",
      "158621   Cn1cc(Nc2nc(NCCC3CC(C)(C)NC3=O)nc(Nc3ccc(C(=O)N[Dy])c(C(=O)O)c3)n2)ccc1=O   \n",
      "\n",
      "       protein_name  binds  \n",
      "89394          BRD4      0  \n",
      "89395           HSA      1  \n",
      "89396           sEH      1  \n",
      "158619         BRD4      1  \n",
      "158620          HSA      0  \n",
      "158621          sEH      0  \n",
      "\n",
      "Verifying validation data:\n",
      "Sampled molecule_smiles from val_data.parquet:\n",
      "['Cc1cc(Nc2nc(NCc3ccccc3N3CCOCC3)nc(N[C@H](Cc3csc4ccccc34)C(=O)N[Dy])n2)ccc1O'\n",
      " 'Cn1cc(Nc2nc(NCC(=O)N3CCCO3)nc(N[C@H](CC(=O)N[Dy])c3cccc([N+](=O)[O-])c3)n2)ccc1=O'\n",
      " 'Cc1ccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H](C(=O)N[Dy])C3CC3)n2)nn1'\n",
      " 'O=C(N[Dy])c1ccc(Nc2nc(Nc3nncs3)nc(Nc3ncnc(Cl)c3Cl)n2)cc1'\n",
      " 'Cc1ccc(C[C@H](Nc2nc(Nc3cc(N4CCNCC4)ccc3[N+](=O)[O-])nc(Nc3n[nH]c4cc(Cl)ccc34)n2)C(=O)N[Dy])cc1'\n",
      " 'O=C(N[Dy])c1ccc(Br)cc1Nc1nc(NCc2cnc3n2CCOC3)nc(Nc2nc3ccc(Cl)cc3s2)n1'\n",
      " 'COc1c(F)cccc1Nc1nc(Nc2ncc(Cl)cc2Cl)nc(N[C@H](Cc2csc3ccccc23)C(=O)N[Dy])n1'\n",
      " 'COC(=O)c1scc(C)c1Nc1nc(Nc2ccc([N+](=O)[O-])c(C(F)(F)F)c2)nc(Nc2cccc(Br)c2C(=O)N[Dy])n1'\n",
      " 'Cn1ccc2cc(Nc3nc(Nc4ncnc(=O)[nH]4)nc(N[C@H](C(=O)N[Dy])C4CCCC4)n3)ccc21'\n",
      " 'Cc1cccc2sc(Nc3nc(Nc4ccc(Cn5ccnc5)cc4)nc(Nc4ccc(C(=O)N[Dy])cc4F)n3)nc12']\n",
      "\n",
      "Rows from val_data.parquet:\n",
      "                                                                                        molecule_smiles  \\\n",
      "1030268          COC(=O)c1scc(C)c1Nc1nc(Nc2ccc([N+](=O)[O-])c(C(F)(F)F)c2)nc(Nc2cccc(Br)c2C(=O)N[Dy])n1   \n",
      "1030269          COC(=O)c1scc(C)c1Nc1nc(Nc2ccc([N+](=O)[O-])c(C(F)(F)F)c2)nc(Nc2cccc(Br)c2C(=O)N[Dy])n1   \n",
      "1133256                       COc1c(F)cccc1Nc1nc(Nc2ncc(Cl)cc2Cl)nc(N[C@H](Cc2csc3ccccc23)C(=O)N[Dy])n1   \n",
      "1133257                       COc1c(F)cccc1Nc1nc(Nc2ncc(Cl)cc2Cl)nc(N[C@H](Cc2csc3ccccc23)C(=O)N[Dy])n1   \n",
      "1695168                     Cc1cc(Nc2nc(NCc3ccccc3N3CCOCC3)nc(N[C@H](Cc3csc4ccccc34)C(=O)N[Dy])n2)ccc1O   \n",
      "1695169                     Cc1cc(Nc2nc(NCc3ccccc3N3CCOCC3)nc(N[C@H](Cc3csc4ccccc34)C(=O)N[Dy])n2)ccc1O   \n",
      "1843432  Cc1ccc(C[C@H](Nc2nc(Nc3cc(N4CCNCC4)ccc3[N+](=O)[O-])nc(Nc3n[nH]c4cc(Cl)ccc34)n2)C(=O)N[Dy])cc1   \n",
      "1843433  Cc1ccc(C[C@H](Nc2nc(Nc3cc(N4CCNCC4)ccc3[N+](=O)[O-])nc(Nc3n[nH]c4cc(Cl)ccc34)n2)C(=O)N[Dy])cc1   \n",
      "1851964                       Cc1ccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H](C(=O)N[Dy])C3CC3)n2)nn1   \n",
      "1851965                       Cc1ccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H](C(=O)N[Dy])C3CC3)n2)nn1   \n",
      "1906324                          Cc1cccc2sc(Nc3nc(Nc4ccc(Cn5ccnc5)cc4)nc(Nc4ccc(C(=O)N[Dy])cc4F)n3)nc12   \n",
      "1906325                          Cc1cccc2sc(Nc3nc(Nc4ccc(Cn5ccnc5)cc4)nc(Nc4ccc(C(=O)N[Dy])cc4F)n3)nc12   \n",
      "2116354               Cn1cc(Nc2nc(NCC(=O)N3CCCO3)nc(N[C@H](CC(=O)N[Dy])c3cccc([N+](=O)[O-])c3)n2)ccc1=O   \n",
      "2116355               Cn1cc(Nc2nc(NCC(=O)N3CCCO3)nc(N[C@H](CC(=O)N[Dy])c3cccc([N+](=O)[O-])c3)n2)ccc1=O   \n",
      "2180654                          Cn1ccc2cc(Nc3nc(Nc4ncnc(=O)[nH]4)nc(N[C@H](C(=O)N[Dy])C4CCCC4)n3)ccc21   \n",
      "2180655                          Cn1ccc2cc(Nc3nc(Nc4ncnc(=O)[nH]4)nc(N[C@H](C(=O)N[Dy])C4CCCC4)n3)ccc21   \n",
      "2841570                            O=C(N[Dy])c1ccc(Br)cc1Nc1nc(NCc2cnc3n2CCOC3)nc(Nc2nc3ccc(Cl)cc3s2)n1   \n",
      "2841571                            O=C(N[Dy])c1ccc(Br)cc1Nc1nc(NCc2cnc3n2CCOC3)nc(Nc2nc3ccc(Cl)cc3s2)n1   \n",
      "2898716                                        O=C(N[Dy])c1ccc(Nc2nc(Nc3nncs3)nc(Nc3ncnc(Cl)c3Cl)n2)cc1   \n",
      "2898717                                        O=C(N[Dy])c1ccc(Nc2nc(Nc3nncs3)nc(Nc3ncnc(Cl)c3Cl)n2)cc1   \n",
      "\n",
      "        binds not_binds  \n",
      "1030268   HSA      BRD4  \n",
      "1030269   HSA       sEH  \n",
      "1133256  BRD4       HSA  \n",
      "1133257  BRD4       sEH  \n",
      "1695168  BRD4       HSA  \n",
      "1695169  BRD4       sEH  \n",
      "1843432  BRD4       HSA  \n",
      "1843433  BRD4       sEH  \n",
      "1851964   sEH      BRD4  \n",
      "1851965   sEH       HSA  \n",
      "1906324   sEH      BRD4  \n",
      "1906325   sEH       HSA  \n",
      "2116354  BRD4       HSA  \n",
      "2116355  BRD4       sEH  \n",
      "2180654   HSA      BRD4  \n",
      "2180655   HSA       sEH  \n",
      "2841570   HSA      BRD4  \n",
      "2841571   HSA       sEH  \n",
      "2898716   HSA      BRD4  \n",
      "2898717   HSA       sEH  \n",
      "\n",
      "Corresponding rows from the original file:\n",
      "                                                                                       molecule_smiles  \\\n",
      "794016  Cc1ccc(C[C@H](Nc2nc(Nc3cc(N4CCNCC4)ccc3[N+](=O)[O-])nc(Nc3n[nH]c4cc(Cl)ccc34)n2)C(=O)N[Dy])cc1   \n",
      "794017  Cc1ccc(C[C@H](Nc2nc(Nc3cc(N4CCNCC4)ccc3[N+](=O)[O-])nc(Nc3n[nH]c4cc(Cl)ccc34)n2)C(=O)N[Dy])cc1   \n",
      "794018  Cc1ccc(C[C@H](Nc2nc(Nc3cc(N4CCNCC4)ccc3[N+](=O)[O-])nc(Nc3n[nH]c4cc(Cl)ccc34)n2)C(=O)N[Dy])cc1   \n",
      "\n",
      "       protein_name  binds  \n",
      "794016         BRD4      1  \n",
      "794017          HSA      0  \n",
      "794018          sEH      0  \n",
      "                                                                     molecule_smiles  \\\n",
      "596950     Cc1ccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H](C(=O)N[Dy])C3CC3)n2)nn1   \n",
      "596951     Cc1ccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H](C(=O)N[Dy])C3CC3)n2)nn1   \n",
      "596952     Cc1ccc(Nc2nc(NCC3(O)C4C5CC6C7C5CC4C7C63)nc(N[C@H](C(=O)N[Dy])C3CC3)n2)nn1   \n",
      "614722        Cn1ccc2cc(Nc3nc(Nc4ncnc(=O)[nH]4)nc(N[C@H](C(=O)N[Dy])C4CCCC4)n3)ccc21   \n",
      "614723        Cn1ccc2cc(Nc3nc(Nc4ncnc(=O)[nH]4)nc(N[C@H](C(=O)N[Dy])C4CCCC4)n3)ccc21   \n",
      "614724        Cn1ccc2cc(Nc3nc(Nc4ncnc(=O)[nH]4)nc(N[C@H](C(=O)N[Dy])C4CCCC4)n3)ccc21   \n",
      "842623     COc1c(F)cccc1Nc1nc(Nc2ncc(Cl)cc2Cl)nc(N[C@H](Cc2csc3ccccc23)C(=O)N[Dy])n1   \n",
      "842624     COc1c(F)cccc1Nc1nc(Nc2ncc(Cl)cc2Cl)nc(N[C@H](Cc2csc3ccccc23)C(=O)N[Dy])n1   \n",
      "842625     COc1c(F)cccc1Nc1nc(Nc2ncc(Cl)cc2Cl)nc(N[C@H](Cc2csc3ccccc23)C(=O)N[Dy])n1   \n",
      "845011   Cc1cc(Nc2nc(NCc3ccccc3N3CCOCC3)nc(N[C@H](Cc3csc4ccccc34)C(=O)N[Dy])n2)ccc1O   \n",
      "845012   Cc1cc(Nc2nc(NCc3ccccc3N3CCOCC3)nc(N[C@H](Cc3csc4ccccc34)C(=O)N[Dy])n2)ccc1O   \n",
      "845013   Cc1cc(Nc2nc(NCc3ccccc3N3CCOCC3)nc(N[C@H](Cc3csc4ccccc34)C(=O)N[Dy])n2)ccc1O   \n",
      "1016317         O=C(N[Dy])c1ccc(Br)cc1Nc1nc(NCc2cnc3n2CCOC3)nc(Nc2nc3ccc(Cl)cc3s2)n1   \n",
      "1016318         O=C(N[Dy])c1ccc(Br)cc1Nc1nc(NCc2cnc3n2CCOC3)nc(Nc2nc3ccc(Cl)cc3s2)n1   \n",
      "1016319         O=C(N[Dy])c1ccc(Br)cc1Nc1nc(NCc2cnc3n2CCOC3)nc(Nc2nc3ccc(Cl)cc3s2)n1   \n",
      "\n",
      "        protein_name  binds  \n",
      "596950          BRD4      0  \n",
      "596951           HSA      0  \n",
      "596952           sEH      1  \n",
      "614722          BRD4      0  \n",
      "614723           HSA      1  \n",
      "614724           sEH      0  \n",
      "842623          BRD4      1  \n",
      "842624           HSA      0  \n",
      "842625           sEH      0  \n",
      "845011          BRD4      1  \n",
      "845012           HSA      0  \n",
      "845013           sEH      0  \n",
      "1016317         BRD4      0  \n",
      "1016318          HSA      1  \n",
      "1016319          sEH      0  \n",
      "                                                                               molecule_smiles  \\\n",
      "274830                                O=C(N[Dy])c1ccc(Nc2nc(Nc3nncs3)nc(Nc3ncnc(Cl)c3Cl)n2)cc1   \n",
      "274831                                O=C(N[Dy])c1ccc(Nc2nc(Nc3nncs3)nc(Nc3ncnc(Cl)c3Cl)n2)cc1   \n",
      "274832                                O=C(N[Dy])c1ccc(Nc2nc(Nc3nncs3)nc(Nc3ncnc(Cl)c3Cl)n2)cc1   \n",
      "309588                  Cc1cccc2sc(Nc3nc(Nc4ccc(Cn5ccnc5)cc4)nc(Nc4ccc(C(=O)N[Dy])cc4F)n3)nc12   \n",
      "309589                  Cc1cccc2sc(Nc3nc(Nc4ccc(Cn5ccnc5)cc4)nc(Nc4ccc(C(=O)N[Dy])cc4F)n3)nc12   \n",
      "309590                  Cc1cccc2sc(Nc3nc(Nc4ccc(Cn5ccnc5)cc4)nc(Nc4ccc(C(=O)N[Dy])cc4F)n3)nc12   \n",
      "504666  COC(=O)c1scc(C)c1Nc1nc(Nc2ccc([N+](=O)[O-])c(C(F)(F)F)c2)nc(Nc2cccc(Br)c2C(=O)N[Dy])n1   \n",
      "504667  COC(=O)c1scc(C)c1Nc1nc(Nc2ccc([N+](=O)[O-])c(C(F)(F)F)c2)nc(Nc2cccc(Br)c2C(=O)N[Dy])n1   \n",
      "504668  COC(=O)c1scc(C)c1Nc1nc(Nc2ccc([N+](=O)[O-])c(C(F)(F)F)c2)nc(Nc2cccc(Br)c2C(=O)N[Dy])n1   \n",
      "997782       Cn1cc(Nc2nc(NCC(=O)N3CCCO3)nc(N[C@H](CC(=O)N[Dy])c3cccc([N+](=O)[O-])c3)n2)ccc1=O   \n",
      "997783       Cn1cc(Nc2nc(NCC(=O)N3CCCO3)nc(N[C@H](CC(=O)N[Dy])c3cccc([N+](=O)[O-])c3)n2)ccc1=O   \n",
      "997784       Cn1cc(Nc2nc(NCC(=O)N3CCCO3)nc(N[C@H](CC(=O)N[Dy])c3cccc([N+](=O)[O-])c3)n2)ccc1=O   \n",
      "\n",
      "       protein_name  binds  \n",
      "274830         BRD4      0  \n",
      "274831          HSA      1  \n",
      "274832          sEH      0  \n",
      "309588         BRD4      0  \n",
      "309589          HSA      0  \n",
      "309590          sEH      1  \n",
      "504666         BRD4      0  \n",
      "504667          HSA      1  \n",
      "504668          sEH      0  \n",
      "997782         BRD4      1  \n",
      "997783          HSA      0  \n",
      "997784          sEH      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "def verify_data(file_path, original_file_path, num_samples=10, chunk_size=100000):\n",
    "    # Load the split data\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Get unique molecule_smiles\n",
    "    unique_molecule_smiles = data['molecule_smiles'].unique()\n",
    "\n",
    "    # Randomly sample 10 molecule_smiles\n",
    "    sampled_smiles = np.random.choice(unique_molecule_smiles, num_samples, replace=False)\n",
    "\n",
    "    print(f\"Sampled molecule_smiles from {file_path}:\")\n",
    "    print(sampled_smiles)\n",
    "\n",
    "    # Print rows corresponding to sampled molecule_smiles from the split file\n",
    "    print(f\"\\nRows from {file_path}:\")\n",
    "    sampled_data = data[data['molecule_smiles'].isin(sampled_smiles)]\n",
    "    print(sampled_data[['molecule_smiles', 'binds', 'not_binds']])\n",
    "\n",
    "    # Read the original file in chunks\n",
    "    print(f\"\\nCorresponding rows from the original file:\")\n",
    "    parquet_file = pq.ParquetFile(original_file_path)\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "\n",
    "    for i in range(num_row_groups):\n",
    "        # Read a batch of rows\n",
    "        row_group = parquet_file.read_row_group(i).to_pandas()\n",
    "\n",
    "        # Filter rows corresponding to sampled molecule_smiles\n",
    "        original_sampled_data = row_group[row_group['molecule_smiles'].isin(sampled_smiles)]\n",
    "        if not original_sampled_data.empty:\n",
    "            print(original_sampled_data[['molecule_smiles', 'protein_name', 'binds']])\n",
    "\n",
    "# File paths\n",
    "train_file_path = 'train_data.parquet'\n",
    "test_file_path = 'test_data.parquet'\n",
    "val_file_path = 'val_data.parquet'\n",
    "original_file_path = 'filtered_train.parquet'\n",
    "\n",
    "# Verify train data\n",
    "print(\"Verifying train data:\")\n",
    "verify_data(train_file_path, original_file_path)\n",
    "\n",
    "# Verify test data\n",
    "print(\"\\nVerifying test data:\")\n",
    "verify_data(test_file_path, original_file_path)\n",
    "\n",
    "# Verify validation data\n",
    "print(\"\\nVerifying validation data:\")\n",
    "verify_data(val_file_path, original_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb266164-524d-4660-83fe-b1c59594caa5",
   "metadata": {},
   "source": [
    "# Exhaustive List of Features for Small Molecules\n",
    "\n",
    "## Molecular Descriptors:\n",
    "- Molecular weight\n",
    "- Number of atoms\n",
    "- Number of bonds\n",
    "- Number of aromatic rings\n",
    "- Number of rotatable bonds\n",
    "- Topological polar surface area (TPSA)\n",
    "- LogP (octanol-water partition coefficient)\n",
    "\n",
    "## Atom-Level Features:\n",
    "- Atom types (e.g., C, H, O, N, S)\n",
    "- Hybridization states (sp, sp2, sp3)\n",
    "- Formal charge\n",
    "- Aromaticity\n",
    "- Degree (number of bonds to the atom)\n",
    "- Implicit and explicit hydrogen counts\n",
    "- Chirality\n",
    "\n",
    "## Bond-Level Features:\n",
    "- Bond types (single, double, triple, aromatic)\n",
    "- Conjugation\n",
    "- Ring membership\n",
    "- Stereo configuration (cis/trans)\n",
    "\n",
    "## Graph-Based Features:\n",
    "- Adjacency matrix\n",
    "- Distance matrix\n",
    "- Graph Laplacian\n",
    "\n",
    "## Physicochemical Properties:\n",
    "- Hydrogen bond donors and acceptors\n",
    "- Molecular refractivity\n",
    "- Molar volume\n",
    "- Electronegativity\n",
    "- Electron affinity\n",
    "\n",
    "## Structural Fingerprints:\n",
    "- MACCS keys\n",
    "- Morgan fingerprints\n",
    "- ECFP (Extended Connectivity Fingerprints)\n",
    "- RDKIT fingerprints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767ee721-a593-45e9-888f-51156582d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, AllChem, rdmolops\n",
    "from rdkit.DataStructs import ConvertToNumpyArray\n",
    "import numpy as np\n",
    "\n",
    "# Define encoding schemes outside the class\n",
    "ATOM_TYPES = ['C', 'H', 'O', 'N', 'S', 'F', 'Cl', 'Br', 'I', 'P', 'B']\n",
    "HYBRIDIZATION_STATES = ['SP', 'SP2', 'SP3', 'SP3D', 'SP3D2']\n",
    "CHIRAL_TAGS = ['CHI_UNSPECIFIED', 'CHI_TETRAHEDRAL_CW', 'CHI_TETRAHEDRAL_CCW', 'CHI_OTHER']\n",
    "BOND_TYPES = ['SINGLE', 'DOUBLE', 'TRIPLE', 'AROMATIC']\n",
    "STEREO_CONFIGURATIONS = ['STEREONONE', 'STEREOZ', 'STEREOE', 'STEREOCIS', 'STEREOTRANS']\n",
    "\n",
    "class SmallMoleculeFeatureExtractor:\n",
    "    def __init__(self, smiles):\n",
    "        self.smiles = smiles\n",
    "        self.mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    def get_molecular_descriptors(self):\n",
    "        descriptors = {\n",
    "            'molecular_weight': Descriptors.MolWt(self.mol),\n",
    "            'num_atoms': self.mol.GetNumAtoms(),\n",
    "            'num_bonds': self.mol.GetNumBonds(),\n",
    "            'num_aromatic_rings': rdMolDescriptors.CalcNumAromaticRings(self.mol),\n",
    "            'num_rotatable_bonds': Descriptors.NumRotatableBonds(self.mol),\n",
    "            'tpsa': Descriptors.TPSA(self.mol),\n",
    "            'logp': Descriptors.MolLogP(self.mol)\n",
    "        }\n",
    "        return descriptors\n",
    "\n",
    "    def one_hot_encode(self, value, categories):\n",
    "        encoding = [0] * len(categories)\n",
    "        if value in categories:\n",
    "            encoding[categories.index(value)] = 1\n",
    "        return encoding\n",
    "\n",
    "    def get_atom_level_features(self):\n",
    "        atom_features = []\n",
    "        for atom in self.mol.GetAtoms():\n",
    "            atom_features.append([\n",
    "                self.one_hot_encode(atom.GetSymbol(), ATOM_TYPES),\n",
    "                self.one_hot_encode(str(atom.GetHybridization()), HYBRIDIZATION_STATES),\n",
    "                atom.GetFormalCharge(),\n",
    "                atom.GetIsAromatic(),\n",
    "                atom.GetDegree(),\n",
    "                atom.GetImplicitValence(),\n",
    "                atom.GetTotalNumHs(),\n",
    "                self.one_hot_encode(str(atom.GetChiralTag()), CHIRAL_TAGS)\n",
    "            ])\n",
    "        return atom_features\n",
    "\n",
    "    def get_bond_level_features(self):\n",
    "        bond_features = []\n",
    "        for bond in self.mol.GetBonds():\n",
    "            bond_features.append([\n",
    "                self.one_hot_encode(str(bond.GetBondType()), BOND_TYPES),\n",
    "                bond.GetIsConjugated(),\n",
    "                bond.IsInRing(),\n",
    "                self.one_hot_encode(str(bond.GetStereo()), STEREO_CONFIGURATIONS)\n",
    "            ])\n",
    "        return bond_features\n",
    "\n",
    "    def get_graph_based_features(self):\n",
    "        adj_matrix = rdmolops.GetAdjacencyMatrix(self.mol)\n",
    "        dist_matrix = rdmolops.GetDistanceMatrix(self.mol)\n",
    "        return {\n",
    "            'adjacency_matrix': adj_matrix,\n",
    "            'distance_matrix': dist_matrix,\n",
    "        }\n",
    "\n",
    "    def get_physicochemical_properties(self):\n",
    "        properties = {\n",
    "            'h_bond_donors': Descriptors.NumHDonors(self.mol),\n",
    "            'h_bond_acceptors': Descriptors.NumHAcceptors(self.mol),\n",
    "            'molecular_refractivity': Descriptors.MolMR(self.mol),\n",
    "            'molar_volume': Descriptors.MolLogP(self.mol) / Descriptors.MolWt(self.mol)\n",
    "        }\n",
    "        return properties\n",
    "\n",
    "    def get_structural_fingerprints(self):\n",
    "        maccs_keys = AllChem.GetMACCSKeysFingerprint(self.mol)\n",
    "        morgan_fp = AllChem.GetMorganFingerprintAsBitVect(self.mol, 2)\n",
    "        rdk_fp = Chem.RDKFingerprint(self.mol)\n",
    "\n",
    "        maccs_keys_np = np.zeros((1,))\n",
    "        ConvertToNumpyArray(maccs_keys, maccs_keys_np)\n",
    "\n",
    "        morgan_fp_np = np.zeros((1,))\n",
    "        ConvertToNumpyArray(morgan_fp, morgan_fp_np)\n",
    "\n",
    "        rdk_fp_np = np.zeros((1,))\n",
    "        ConvertToNumpyArray(rdk_fp, rdk_fp_np)\n",
    "        \n",
    "        return {\n",
    "            'maccs_keys': maccs_keys_np,\n",
    "            'morgan_fp': morgan_fp_np,\n",
    "            'rdkit_fp': rdk_fp_np\n",
    "        }\n",
    "\n",
    "    def extract_features(self):\n",
    "        features = {\n",
    "            'molecular_descriptors': self.get_molecular_descriptors(),\n",
    "            'atom_level_features': self.get_atom_level_features(),\n",
    "            'bond_level_features': self.get_bond_level_features(),\n",
    "            'graph_based_features': self.get_graph_based_features(),\n",
    "            'physicochemical_properties': self.get_physicochemical_properties(),\n",
    "            'structural_fingerprints': self.get_structural_fingerprints()\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def flatten_features(self):\n",
    "        # Extract individual features\n",
    "        molecular_descriptors = self.get_molecular_descriptors()\n",
    "        physicochemical_properties = self.get_physicochemical_properties()\n",
    "        structural_fingerprints = self.get_structural_fingerprints()\n",
    "        graph_based_features = self.get_graph_based_features()\n",
    "    \n",
    "        # Flatten the structural fingerprints\n",
    "        flattened_structural_fingerprints = np.concatenate([\n",
    "            structural_fingerprints['maccs_keys'],\n",
    "            structural_fingerprints['morgan_fp'],\n",
    "            structural_fingerprints['rdkit_fp']\n",
    "        ])\n",
    "    \n",
    "        # Convert molecular descriptors and physicochemical properties to arrays\n",
    "        molecular_descriptors_array = np.array(list(molecular_descriptors.values()))\n",
    "        physicochemical_properties_array = np.array(list(physicochemical_properties.values()))\n",
    "\n",
    "        # Extract adjacency and distance matrices\n",
    "        adjacency_matrix = graph_based_features['adjacency_matrix']\n",
    "        distance_matrix = graph_based_features['distance_matrix']\n",
    "    \n",
    "        return {\n",
    "            'molecular_descriptors': molecular_descriptors_array,\n",
    "            'physicochemical_properties': physicochemical_properties_array,\n",
    "            'structural_fingerprints': flattened_structural_fingerprints,\n",
    "            'adjacency_matrix': adjacency_matrix,\n",
    "            'distance_matrix': distance_matrix\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0a1be6c-f0a2-4f50-803b-3d3ab03a60bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molecular_descriptors: [349.386   26.      28.       2.       6.      75.63     3.3917]\n",
      "physicochemical_properties: [2.00000000e+00 3.00000000e+00 9.76955000e+01 9.70760133e-03]\n",
      "structural_fingerprints: [0. 0. 0. ... 1. 1. 1.]\n",
      "adjacency_matrix: [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0]]\n",
      "distance_matrix: [[ 0.  1.  2.  3.  4.  5.  6.  6.  4.  5.  6.  6.  7.  8.  9. 10. 11. 12.\n",
      "  11. 10. 10. 11. 12. 11. 10.  9.]\n",
      " [ 1.  0.  1.  2.  3.  4.  5.  5.  3.  4.  5.  5.  6.  7.  8.  9. 10. 11.\n",
      "  10.  9.  9. 10. 11. 10.  9.  8.]\n",
      " [ 2.  1.  0.  1.  2.  3.  4.  4.  2.  3.  4.  4.  5.  6.  7.  8.  9. 10.\n",
      "   9.  8.  8.  9. 10.  9.  8.  7.]\n",
      " [ 3.  2.  1.  0.  1.  2.  3.  3.  1.  2.  3.  3.  4.  5.  6.  7.  8.  9.\n",
      "   8.  7.  7.  8.  9.  8.  7.  6.]\n",
      " [ 4.  3.  2.  1.  0.  1.  2.  2.  2.  3.  4.  4.  5.  6.  7.  8.  9. 10.\n",
      "   9.  8.  8.  9. 10.  9.  8.  7.]\n",
      " [ 5.  4.  3.  2.  1.  0.  1.  1.  3.  4.  5.  5.  6.  7.  8.  9. 10. 11.\n",
      "  10.  9.  9. 10. 11. 10.  9.  8.]\n",
      " [ 6.  5.  4.  3.  2.  1.  0.  2.  4.  5.  6.  6.  7.  8.  9. 10. 11. 12.\n",
      "  11. 10. 10. 11. 12. 11. 10.  9.]\n",
      " [ 6.  5.  4.  3.  2.  1.  2.  0.  4.  5.  6.  6.  7.  8.  9. 10. 11. 12.\n",
      "  11. 10. 10. 11. 12. 11. 10.  9.]\n",
      " [ 4.  3.  2.  1.  2.  3.  4.  4.  0.  1.  2.  2.  3.  4.  5.  6.  7.  8.\n",
      "   7.  6.  6.  7.  8.  7.  6.  5.]\n",
      " [ 5.  4.  3.  2.  3.  4.  5.  5.  1.  0.  1.  1.  2.  3.  4.  5.  6.  7.\n",
      "   6.  5.  5.  6.  7.  6.  5.  4.]\n",
      " [ 6.  5.  4.  3.  4.  5.  6.  6.  2.  1.  0.  2.  3.  4.  5.  6.  7.  8.\n",
      "   7.  6.  6.  7.  8.  7.  6.  5.]\n",
      " [ 6.  5.  4.  3.  4.  5.  6.  6.  2.  1.  2.  0.  1.  2.  3.  4.  5.  6.\n",
      "   5.  4.  4.  5.  6.  5.  4.  3.]\n",
      " [ 7.  6.  5.  4.  5.  6.  7.  7.  3.  2.  3.  1.  0.  1.  2.  3.  4.  5.\n",
      "   4.  3.  3.  4.  5.  4.  3.  2.]\n",
      " [ 8.  7.  6.  5.  6.  7.  8.  8.  4.  3.  4.  2.  1.  0.  1.  2.  3.  4.\n",
      "   3.  2.  2.  3.  4.  3.  2.  1.]\n",
      " [ 9.  8.  7.  6.  7.  8.  9.  9.  5.  4.  5.  3.  2.  1.  0.  1.  2.  3.\n",
      "   2.  1.  2.  3.  4.  4.  3.  2.]\n",
      " [10.  9.  8.  7.  8.  9. 10. 10.  6.  5.  6.  4.  3.  2.  1.  0.  1.  2.\n",
      "   3.  2.  3.  4.  5.  5.  4.  3.]\n",
      " [11. 10.  9.  8.  9. 10. 11. 11.  7.  6.  7.  5.  4.  3.  2.  1.  0.  1.\n",
      "   2.  3.  4.  5.  6.  6.  5.  4.]\n",
      " [12. 11. 10.  9. 10. 11. 12. 12.  8.  7.  8.  6.  5.  4.  3.  2.  1.  0.\n",
      "   1.  2.  3.  4.  5.  6.  5.  4.]\n",
      " [11. 10.  9.  8.  9. 10. 11. 11.  7.  6.  7.  5.  4.  3.  2.  3.  2.  1.\n",
      "   0.  1.  2.  3.  4.  5.  4.  3.]\n",
      " [10.  9.  8.  7.  8.  9. 10. 10.  6.  5.  6.  4.  3.  2.  1.  2.  3.  2.\n",
      "   1.  0.  1.  2.  3.  4.  3.  2.]\n",
      " [10.  9.  8.  7.  8.  9. 10. 10.  6.  5.  6.  4.  3.  2.  2.  3.  4.  3.\n",
      "   2.  1.  0.  1.  2.  3.  2.  1.]\n",
      " [11. 10.  9.  8.  9. 10. 11. 11.  7.  6.  7.  5.  4.  3.  3.  4.  5.  4.\n",
      "   3.  2.  1.  0.  1.  2.  3.  2.]\n",
      " [12. 11. 10.  9. 10. 11. 12. 12.  8.  7.  8.  6.  5.  4.  4.  5.  6.  5.\n",
      "   4.  3.  2.  1.  0.  1.  2.  3.]\n",
      " [11. 10.  9.  8.  9. 10. 11. 11.  7.  6.  7.  5.  4.  3.  4.  5.  6.  6.\n",
      "   5.  4.  3.  2.  1.  0.  1.  2.]\n",
      " [10.  9.  8.  7.  8.  9. 10. 10.  6.  5.  6.  4.  3.  2.  3.  4.  5.  5.\n",
      "   4.  3.  2.  3.  2.  1.  0.  1.]\n",
      " [ 9.  8.  7.  6.  7.  8.  9.  9.  5.  4.  5.  3.  2.  1.  2.  3.  4.  4.\n",
      "   3.  2.  1.  2.  3.  2.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "smiles = \"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\"\n",
    "extractor = SmallMoleculeFeatureExtractor(smiles)\n",
    "features = extractor.flatten_features()\n",
    "for feature, value in features.items():\n",
    "    print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fca0bf-cb35-4f82-bbd5-0f24f2bf50aa",
   "metadata": {},
   "source": [
    "# Feature Extraction from Protein Structure PDB File\n",
    "\n",
    "## Structural Features\n",
    "\n",
    "### Amino Acid Composition:\n",
    "- Frequency of each amino acid type in the binding site.\n",
    "- Frequency of amino acid types in the entire protein.\n",
    "\n",
    "### Secondary Structure:\n",
    "- Percentage of alpha-helices, beta-sheets, and random coils in the binding site.\n",
    "- Secondary structure elements around the binding site.\n",
    "\n",
    "### Tertiary Structure:\n",
    "- 3D coordinates of the binding site.\n",
    "- Distance between key residues in the binding site.\n",
    "\n",
    "### Binding Site Characteristics:\n",
    "- Volume and surface area of the binding site.\n",
    "- Shape descriptors (e.g., sphericity, elongation).\n",
    "\n",
    "## Physicochemical Properties\n",
    "\n",
    "### Hydrophobicity:\n",
    "- Hydrophobic and hydrophilic residue distribution in the binding site.\n",
    "- Hydrophobic surface area.\n",
    "\n",
    "### Charge Distribution:\n",
    "- Number and type of charged residues (positive and negative).\n",
    "- Electrostatic potential distribution.\n",
    "\n",
    "### Polarity:\n",
    "- Number of polar residues.\n",
    "- Polar surface area.\n",
    "\n",
    "### Solvent Accessibility:\n",
    "- Solvent-accessible surface area (SASA) of residues in the binding site.\n",
    "\n",
    "### Hydrogen Bonding:\n",
    "- Number of potential hydrogen bond donors and acceptors.\n",
    "- Hydrogen bond network in the binding site.\n",
    "\n",
    "### Van der Waals Interactions:\n",
    "- Van der Waals interaction potential of the binding site.\n",
    "\n",
    "## Geometric Features\n",
    "\n",
    "### Distance Metrics:\n",
    "- Pairwise distances between all residues in the binding site.\n",
    "- Distance to the nearest surface residue.\n",
    "\n",
    "### Angles and Dihedrals:\n",
    "- Angles and dihedral angles between residues in the binding site.\n",
    "\n",
    "## Chemical Environment\n",
    "\n",
    "### Residue Environment:\n",
    "- Local chemical environment of each residue (e.g., neighboring residues within a certain radius).\n",
    "\n",
    "### Ligand Interaction Sites:\n",
    "- Specific interaction sites for known ligands (if available).\n",
    "\n",
    "## Dynamic Properties\n",
    "\n",
    "### Flexibility:\n",
    "- B-factors or temperature factors indicating residue flexibility.\n",
    "\n",
    "### Molecular Dynamics Simulations:\n",
    "- Root mean square fluctuation (RMSF) of residues in the binding site.\n",
    "- Conformational changes over time.\n",
    "\n",
    "## Topological Features\n",
    "\n",
    "### Graph-based Features:\n",
    "- Protein structure represented as a graph with nodes (residues) and edges (interactions).\n",
    "- Degree centrality, betweenness centrality, and clustering coefficient of residues in the binding site.\n",
    "\n",
    "## Energy-based Features\n",
    "\n",
    "### Binding Energy:\n",
    "- Estimated binding free energy of known ligands.\n",
    "- Energy components (van der Waals, electrostatic, solvation) from docking simulations.\n",
    "\n",
    "## Protein-Ligand Interaction Features\n",
    "\n",
    "### Docking Scores:\n",
    "- Scores from molecular docking simulations with various ligands.\n",
    "\n",
    "### Interaction Profiles:\n",
    "- Interaction fingerprints summarizing the types and strengths of interactions with ligands.\n",
    "\n",
    "## Evolutionary Features\n",
    "\n",
    "### Conservation:\n",
    "- Sequence conservation of residues in the binding site (e.g., from multiple sequence alignment).\n",
    "\n",
    "### Mutational Impact:\n",
    "- Predicted impact of mutations on binding site residues.\n",
    "\n",
    "## Experimental Data\n",
    "\n",
    "### Experimental Binding Data:\n",
    "- Known binding affinities (e.g., Kd, Ki, IC50) for small molecules.\n",
    "\n",
    "## Contextual Features\n",
    "\n",
    "### Functional Annotations:\n",
    "- Biological function and pathway involvement of the protein.\n",
    "- Known protein-protein interactions.\n",
    "\n",
    "## Integration and Representation\n",
    "\n",
    "### Feature Scaling and Normalization:\n",
    "- Standardize and normalize features for input into the deep learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49e9f49b-65e5-40ff-9103-a170239f38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser, is_aa, NeighborSearch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "class ProteinFeatureExtractor:\n",
    "    def __init__(self, pdb_file):\n",
    "        self.pdb_file = pdb_file\n",
    "        self.structure = self.load_structure()\n",
    "        self.ligand_resnames = self.detect_ligands()\n",
    "        self.graph = self.construct_graph()\n",
    "\n",
    "    def load_structure(self):\n",
    "        # Load the PDB structure\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure('protein', self.pdb_file)\n",
    "        return structure\n",
    "\n",
    "    def detect_ligands(self):\n",
    "        # Detect ligand residue names by excluding standard amino acids and water\n",
    "        ligands = set()\n",
    "        standard_amino_acids = {'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE', \n",
    "                                'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL'}\n",
    "        water_residues = {'HOH'}\n",
    "        for residue in self.structure.get_residues():\n",
    "            resname = residue.resname\n",
    "            if resname not in standard_amino_acids and resname not in water_residues:\n",
    "                ligands.add(resname)\n",
    "        return list(ligands)\n",
    "\n",
    "    def get_amino_acid_composition(self):\n",
    "        # Get the composition of amino acids in the protein\n",
    "        amino_acids = [residue.resname for residue in self.structure.get_residues() if residue.id[0] == ' ']\n",
    "        aa_counts = {aa: amino_acids.count(aa) for aa in set(amino_acids)}\n",
    "        return aa_counts\n",
    "\n",
    "\n",
    "    def get_flexibility(self):\n",
    "        # Calculate the flexibility of the protein based on B-factors\n",
    "        flexibility = []\n",
    "        for atom in self.structure.get_atoms():\n",
    "            flexibility.append(atom.bfactor)\n",
    "        return np.mean(flexibility)\n",
    "\n",
    "    def get_distance_metrics(self):\n",
    "        # Calculate distance metrics between residues in the protein\n",
    "        distances = []\n",
    "        for chain in self.structure.get_chains():\n",
    "            print(f\"Processing chain: {chain.id}\")\n",
    "            residues = [res for res in chain if 'CA' in res.child_dict]  # Filter residues with 'CA' atom\n",
    "            for i, res1 in enumerate(residues):\n",
    "                ca1 = res1.child_dict.get('CA')\n",
    "                if ca1 is None:\n",
    "                    print(f\"Residue {res1} does not have a CA atom.\")\n",
    "                    continue\n",
    "                for j, res2 in enumerate(residues):\n",
    "                    if i < j:\n",
    "                        ca2 = res2.child_dict.get('CA')\n",
    "                        if ca2 is None:\n",
    "                            print(f\"Residue {res2} does not have a CA atom.\")\n",
    "                            continue\n",
    "                        try:\n",
    "                            distance = ca1 - ca2\n",
    "                            distances.append(distance)\n",
    "                        except KeyError as e:\n",
    "                            print(f\"Error calculating distance: {e}\")\n",
    "        return distances\n",
    "\n",
    "    def construct_graph(self, cutoff=4.0):\n",
    "        # Initialize an undirected graph\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Add nodes for each residue\n",
    "        for chain in self.structure.get_chains():\n",
    "            for residue in chain:\n",
    "                if is_aa(residue):\n",
    "                    G.add_node(residue.id, residue=residue)\n",
    "\n",
    "        # Add edges based on distance cutoff\n",
    "        atoms = list(self.structure.get_atoms())\n",
    "        ns = NeighborSearch(atoms)\n",
    "        for atom in atoms:\n",
    "            if atom.element == 'H':  # Skip hydrogen atoms\n",
    "                continue\n",
    "            neighbors = ns.search(atom.coord, cutoff)\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor.element == 'H':  # Skip hydrogen atoms\n",
    "                    continue\n",
    "                res1 = atom.get_parent()\n",
    "                res2 = neighbor.get_parent()\n",
    "                if res1 != res2:\n",
    "                    G.add_edge(res1.id, res2.id, weight=atom - neighbor)\n",
    "\n",
    "        return G\n",
    "\n",
    "    def extract_graph_features(self):\n",
    "        # Adjacency matrix\n",
    "        adjacency_matrix = nx.adjacency_matrix(self.graph).todense()\n",
    "\n",
    "        # Distance matrix (Floyd-Warshall algorithm)\n",
    "        distance_matrix = nx.floyd_warshall_numpy(self.graph)\n",
    "\n",
    "        # Degree centrality\n",
    "        degree_centrality = nx.degree_centrality(self.graph)\n",
    "\n",
    "        # Betweenness centrality\n",
    "        betweenness_centrality = nx.betweenness_centrality(self.graph)\n",
    "\n",
    "        # Clustering coefficient\n",
    "        clustering_coefficient = nx.clustering(self.graph)\n",
    "\n",
    "        # Ensure features are in a consistent order\n",
    "        nodes = list(self.graph.nodes)\n",
    "        degree_centrality = np.array([degree_centrality[node] for node in nodes])\n",
    "        betweenness_centrality = np.array([betweenness_centrality[node] for node in nodes])\n",
    "        clustering_coefficient = np.array([clustering_coefficient[node] for node in nodes])\n",
    "\n",
    "        # Aggregate features into a dictionary\n",
    "        features = {\n",
    "            'adjacency_matrix': adjacency_matrix,\n",
    "            'distance_matrix': distance_matrix,\n",
    "            'degree_centrality': degree_centrality,\n",
    "            'betweenness_centrality': betweenness_centrality,\n",
    "            'clustering_coefficient': clustering_coefficient\n",
    "        }\n",
    "\n",
    "        return features\n",
    "\n",
    "    def extract_features(self):\n",
    "        # Extract various features from the protein structure\n",
    "        amino_acid_composition = self.get_amino_acid_composition()\n",
    "        flexibility = self.get_flexibility()\n",
    "        distance_metrics = self.get_distance_metrics()\n",
    "        graph_features = self.extract_graph_features()\n",
    "\n",
    "        features = {\n",
    "            \"amino_acid_composition\": amino_acid_composition,\n",
    "            \"flexibility\": flexibility,\n",
    "            \"distance_metrics\": distance_metrics,\n",
    "            \"graph_features\": graph_features\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_and_aggregate_features(self):\n",
    "        # Extract various features from the protein structure\n",
    "        amino_acid_composition = self.get_amino_acid_composition()\n",
    "        flexibility = self.get_flexibility()\n",
    "        distance_metrics = self.get_distance_metrics()\n",
    "        graph_features = self.extract_graph_features()\n",
    "    \n",
    "        # Combine amino acid composition and flexibility into a single array\n",
    "        amino_acid_comp_values = list(amino_acid_composition.values())\n",
    "        combined_features = amino_acid_comp_values + [flexibility]\n",
    "    \n",
    "        features = {\n",
    "            \"protein_combined_features\": np.array(combined_features),\n",
    "            \"distance_metrics\": distance_metrics,\n",
    "            \"distance_matrix\": graph_features[\"distance_matrix\"],\n",
    "            \"adjacency_matrix\": graph_features[\"adjacency_matrix\"],\n",
    "            \"degree_centrality\": graph_features[\"degree_centrality\"],\n",
    "            \"betweenness_centrality\": graph_features[\"betweenness_centrality\"],\n",
    "            \"clustering_coefficient\": graph_features[\"clustering_coefficient\"],\n",
    "        }\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d39ea1a6-9e37-4247-b1b2-11ad9b98ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushaldsouza/miniconda3/envs/dgl/lib/python3.12/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 9766.\n",
      "  warnings.warn(\n",
      "/Users/kushaldsouza/miniconda3/envs/dgl/lib/python3.12/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 9769.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chain: A\n",
      "Processing chain: B\n",
      "[[0.        3.6402905 3.8100557 ... 0.        0.        0.       ]\n",
      " [3.6402905 0.        3.7282476 ... 0.        0.        0.       ]\n",
      " [3.8100557 3.7282476 0.        ... 0.        0.        0.       ]\n",
      " ...\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "pdb_file = \"./ALB.pdb\"\n",
    "extractor = ProteinFeatureExtractor(pdb_file)\n",
    "aggregated_features = extractor.extract_and_aggregate_features()\n",
    "print(aggregated_features[\"adjacency_matrix\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ed8106-5cb8-4e0a-a43e-52e8493085be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all data\n",
      "Completed loading all data\n",
      "Final sample size 150974\n",
      "Starting protein node creation\n",
      "Processing chain: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushaldsouza/miniconda3/envs/dgl/lib/python3.12/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 9766.\n",
      "  warnings.warn(\n",
      "/Users/kushaldsouza/miniconda3/envs/dgl/lib/python3.12/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 9769.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chain: A\n",
      "Processing chain: B\n",
      "Processing chain: A\n",
      "Completed protein node creation\n",
      "Starting processing samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▎                      | 53103/150974 [01:36<02:57, 549.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 281\u001b[0m\n\u001b[1;32m    273\u001b[0m protein_files \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBRD4\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./BRD4.pdb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHSA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./ALB.pdb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msEH\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./EPH.pdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    277\u001b[0m }\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# train_pos_g, train_neg_g = create_heterogeneous_graphs(train_file_path, protein_files, 'train')\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# test_pos_g, test_neg_g = create_heterogeneous_graphs(test_file_path, protein_files, 'test')\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m val_pos_g, val_neg_g \u001b[38;5;241m=\u001b[39m create_heterogeneous_graphs(val_file_path, protein_files, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 133\u001b[0m, in \u001b[0;36mcreate_heterogeneous_graphs\u001b[0;34m(parquet_file_path, protein_files, variant)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Process small molecule\u001b[39;00m\n\u001b[1;32m    132\u001b[0m mol_extractor \u001b[38;5;241m=\u001b[39m SmallMoleculeFeatureExtractor(molecule_smiles)\n\u001b[0;32m--> 133\u001b[0m mol_features \u001b[38;5;241m=\u001b[39m mol_extractor\u001b[38;5;241m.\u001b[39mflatten_features()\n\u001b[1;32m    134\u001b[0m mol_idx \u001b[38;5;241m=\u001b[39m node_indices[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmall_molecule\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    135\u001b[0m mol_desc_index \u001b[38;5;241m=\u001b[39m node_indices[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmolecular_descriptor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[9], line 114\u001b[0m, in \u001b[0;36mSmallMoleculeFeatureExtractor.flatten_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m molecular_descriptors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_molecular_descriptors()\n\u001b[1;32m    113\u001b[0m physicochemical_properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_physicochemical_properties()\n\u001b[0;32m--> 114\u001b[0m structural_fingerprints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_structural_fingerprints()\n\u001b[1;32m    115\u001b[0m graph_based_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_graph_based_features()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Flatten the structural fingerprints\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 80\u001b[0m, in \u001b[0;36mSmallMoleculeFeatureExtractor.get_structural_fingerprints\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_structural_fingerprints\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     maccs_keys \u001b[38;5;241m=\u001b[39m AllChem\u001b[38;5;241m.\u001b[39mGetMACCSKeysFingerprint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmol)\n\u001b[1;32m     81\u001b[0m     morgan_fp \u001b[38;5;241m=\u001b[39m AllChem\u001b[38;5;241m.\u001b[39mGetMorganFingerprintAsBitVect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmol, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     82\u001b[0m     rdk_fp \u001b[38;5;241m=\u001b[39m Chem\u001b[38;5;241m.\u001b[39mRDKFingerprint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmol)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "# Assume SmallMoleculeFeatureExtractor and ProteinFeatureExtractor classes are already defined.\n",
    "\n",
    "def create_heterogeneous_graphs(parquet_file_path, protein_files, variant):\n",
    "    # Load parquet file and initialize unique sets\n",
    "    parquet_file = pq.ParquetFile(parquet_file_path)\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "\n",
    "    all_data = []\n",
    "    print('Loading all data')\n",
    "    for i in range(num_row_groups):\n",
    "        row_group = parquet_file.read_row_group(i).to_pandas()\n",
    "        all_data.append(row_group)\n",
    "    all_data = pd.concat(all_data, ignore_index=True)\n",
    "    print('Completed loading all data')\n",
    "\n",
    "    print(f'Final sample size {len(all_data)}')\n",
    "\n",
    "    # Initialize node and edge lists for positive and negative graphs\n",
    "    node_data = {\n",
    "        'building_block': [],\n",
    "        'small_molecule': [],\n",
    "        'molecular_descriptor': [],\n",
    "        'physicochemical_properties': [],\n",
    "        'adjacency_matrix': [],\n",
    "        'distance_matrix': [],\n",
    "        'protein': [],\n",
    "        'distance_metrics': [],\n",
    "        'degree_centrality': [],\n",
    "        'betweenness_centrality': [],\n",
    "        'clustering_coefficient': []\n",
    "    }\n",
    "\n",
    "    edge_data_pos = {\n",
    "        ('building_block', 'has', 'molecular_descriptor'): [],\n",
    "        ('building_block', 'has', 'physicochemical_properties'): [],\n",
    "        ('building_block', 'has', 'adjacency_matrix'): [],\n",
    "        ('building_block', 'has', 'distance_matrix'): [],\n",
    "        ('small_molecule', 'has', 'molecular_descriptor'): [],\n",
    "        ('small_molecule', 'has', 'physicochemical_properties'): [],\n",
    "        ('small_molecule', 'has', 'adjacency_matrix'): [],\n",
    "        ('small_molecule', 'has', 'distance_matrix'): [],\n",
    "        ('small_molecule', 'contains', 'building_block'): [],\n",
    "        ('small_molecule', 'binds', 'protein'): [],\n",
    "        ('protein', 'has', 'distance_metrics'): [],\n",
    "        ('protein', 'has', 'degree_centrality'): [],\n",
    "        ('protein', 'has', 'betweenness_centrality'): [],\n",
    "        ('protein', 'has', 'clustering_coefficient'): []\n",
    "    }\n",
    "\n",
    "    edge_data_neg = {\n",
    "        ('building_block', 'has', 'molecular_descriptor'): [],\n",
    "        ('building_block', 'has', 'physicochemical_properties'): [],\n",
    "        ('building_block', 'has', 'adjacency_matrix'): [],\n",
    "        ('building_block', 'has', 'distance_matrix'): [],\n",
    "        ('small_molecule', 'has', 'molecular_descriptor'): [],\n",
    "        ('small_molecule', 'has', 'physicochemical_properties'): [],\n",
    "        ('small_molecule', 'has', 'adjacency_matrix'): [],\n",
    "        ('small_molecule', 'has', 'distance_matrix'): [],\n",
    "        ('small_molecule', 'contains', 'building_block'): [],\n",
    "        ('small_molecule', 'binds', 'protein'): [],\n",
    "        ('protein', 'has', 'distance_metrics'): [],\n",
    "        ('protein', 'has', 'degree_centrality'): [],\n",
    "        ('protein', 'has', 'betweenness_centrality'): [],\n",
    "        ('protein', 'has', 'clustering_coefficient'): []\n",
    "    }\n",
    "\n",
    "    # Dictionaries to keep track of indices and for saving later\n",
    "    node_indices = {ntype: 0 for ntype in node_data.keys()}\n",
    "    building_block_index_map = {}\n",
    "    protein_index_map = {}\n",
    "    \n",
    "    print('Starting protein node creation')\n",
    "    # Load protein data\n",
    "    for protein_name, pdb_file in protein_files.items():\n",
    "        extractor = ProteinFeatureExtractor(pdb_file)\n",
    "        features = extractor.extract_and_aggregate_features()\n",
    "        protein_idx = node_indices['protein']\n",
    "        node_data['protein'].append((protein_idx, {'feature': torch.tensor(features['protein_combined_features'])}))\n",
    "        node_data['distance_metrics'].append((protein_idx, {'feature': torch.tensor(features['distance_metrics'])}))\n",
    "        node_data['degree_centrality'].append((protein_idx, {'feature': torch.tensor(features['degree_centrality'])}))\n",
    "        node_data['betweenness_centrality'].append((protein_idx, {'feature': torch.tensor(features['betweenness_centrality'])}))\n",
    "        node_data['clustering_coefficient'].append((protein_idx, {'feature': torch.tensor(features['clustering_coefficient'])}))\n",
    "\n",
    "        #Add edges for protein features\n",
    "        edge_data_pos[('protein', 'has', 'distance_metrics')].append((protein_idx, protein_idx))\n",
    "        edge_data_pos[('protein', 'has', 'degree_centrality')].append((protein_idx, protein_idx))\n",
    "        edge_data_pos[('protein', 'has', 'betweenness_centrality')].append((protein_idx, protein_idx))\n",
    "        edge_data_pos[('protein', 'has', 'clustering_coefficient')].append((protein_idx, protein_idx))\n",
    "\n",
    "        edge_data_neg[('protein', 'has', 'distance_metrics')].append((protein_idx, protein_idx))\n",
    "        edge_data_neg[('protein', 'has', 'degree_centrality')].append((protein_idx, protein_idx))\n",
    "        edge_data_neg[('protein', 'has', 'betweenness_centrality')].append((protein_idx, protein_idx))\n",
    "        edge_data_neg[('protein', 'has', 'clustering_coefficient')].append((protein_idx, protein_idx))\n",
    "        \n",
    "        protein_index_map[protein_name] = protein_idx\n",
    "        node_indices['protein'] += 1\n",
    "\n",
    "    print('Completed protein node creation')\n",
    "\n",
    "    # Process the sample\n",
    "    print('Starting processing samples')\n",
    "    start_time = time.time()\n",
    "    log_interval = 120\n",
    "    total_rows = all_data.shape[0]\n",
    "\n",
    "    for sampleIndex, row in tqdm(all_data.iterrows(), total=len(all_data)):\n",
    "        # Check elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time >= log_interval:\n",
    "            print(f\"Processing index: {sampleIndex} of {total_rows}\")\n",
    "            start_time = time.time()  # Reset start time\n",
    "\n",
    "        building_blocks = [\n",
    "            row['buildingblock1_smiles'],\n",
    "            row['buildingblock2_smiles'],\n",
    "            row['buildingblock3_smiles']\n",
    "        ]\n",
    "        molecule_smiles = row['molecule_smiles']\n",
    "        binds_protein = row['binds']\n",
    "        not_binds_protein = row['not_binds']\n",
    "\n",
    "        # Process small molecule\n",
    "        mol_extractor = SmallMoleculeFeatureExtractor(molecule_smiles)\n",
    "        mol_features = mol_extractor.flatten_features()\n",
    "        mol_idx = node_indices['small_molecule']\n",
    "        mol_desc_index = node_indices['molecular_descriptor']\n",
    "        phys_index = node_indices['physicochemical_properties']\n",
    "        adj_index = node_indices['adjacency_matrix']\n",
    "        dist_index = node_indices['distance_matrix']\n",
    "\n",
    "        node_data['small_molecule'].append((mol_idx, {'feature': torch.tensor(mol_features['structural_fingerprints'])}))\n",
    "        node_data['molecular_descriptor'].append((mol_desc_index, {'feature': torch.tensor(mol_features['molecular_descriptors'])}))\n",
    "        node_data['physicochemical_properties'].append((phys_index, {'feature': torch.tensor(mol_features['physicochemical_properties'])}))\n",
    "        node_data['adjacency_matrix'].append((adj_index, {'feature': torch.tensor(mol_features['adjacency_matrix'])}))\n",
    "        node_data['distance_matrix'].append((dist_index, {'feature': torch.tensor(mol_features['distance_matrix'])}))\n",
    "\n",
    "        node_indices['small_molecule'] += 1\n",
    "        node_indices['molecular_descriptor'] += 1\n",
    "        node_indices['physicochemical_properties'] += 1\n",
    "        node_indices['adjacency_matrix'] += 1\n",
    "        node_indices['distance_matrix'] += 1\n",
    "\n",
    "        # Create 'has' edges for small molecule\n",
    "        edge_data_pos[('small_molecule', 'has', 'molecular_descriptor')].append((mol_idx, mol_desc_index))\n",
    "        edge_data_pos[('small_molecule', 'has', 'physicochemical_properties')].append((mol_idx, phys_index))\n",
    "        edge_data_pos[('small_molecule', 'has', 'adjacency_matrix')].append((mol_idx, adj_index))\n",
    "        edge_data_pos[('small_molecule', 'has', 'distance_matrix')].append((mol_idx, dist_index))\n",
    "\n",
    "        edge_data_neg[('small_molecule', 'has', 'molecular_descriptor')].append((mol_idx, mol_desc_index))\n",
    "        edge_data_neg[('small_molecule', 'has', 'physicochemical_properties')].append((mol_idx, phys_index))\n",
    "        edge_data_neg[('small_molecule', 'has', 'adjacency_matrix')].append((mol_idx, adj_index))\n",
    "        edge_data_neg[('small_molecule', 'has', 'distance_matrix')].append((mol_idx, dist_index))\n",
    "\n",
    "        # Process building blocks\n",
    "        for bb_smiles in building_blocks:\n",
    "            if bb_smiles not in building_block_index_map:\n",
    "                bb_extractor = SmallMoleculeFeatureExtractor(bb_smiles)\n",
    "                bb_features = bb_extractor.flatten_features()\n",
    "                bb_idx = node_indices['building_block']\n",
    "                mol_desc_index = node_indices['molecular_descriptor']\n",
    "                phys_index = node_indices['physicochemical_properties']\n",
    "                adj_index = node_indices['adjacency_matrix']\n",
    "                dist_index = node_indices['distance_matrix']\n",
    "    \n",
    "                node_data['building_block'].append((bb_idx, {'feature': torch.tensor(bb_features['structural_fingerprints'])}))\n",
    "                node_data['molecular_descriptor'].append((mol_desc_index, {'feature': torch.tensor(bb_features['molecular_descriptors'])}))\n",
    "                node_data['physicochemical_properties'].append((phys_index, {'feature': torch.tensor(bb_features['physicochemical_properties'])}))\n",
    "                node_data['adjacency_matrix'].append((adj_index, {'feature': torch.tensor(bb_features['adjacency_matrix'])}))\n",
    "                node_data['distance_matrix'].append((dist_index, {'feature': torch.tensor(bb_features['distance_matrix'])}))\n",
    "\n",
    "                node_indices['building_block'] += 1\n",
    "                node_indices['molecular_descriptor'] += 1\n",
    "                node_indices['physicochemical_properties'] += 1\n",
    "                node_indices['adjacency_matrix'] += 1\n",
    "                node_indices['distance_matrix'] += 1\n",
    "\n",
    "                # Create 'has' edges for building block\n",
    "                edge_data_pos[('building_block', 'has', 'molecular_descriptor')].append((bb_idx, mol_desc_index))\n",
    "                edge_data_pos[('building_block', 'has', 'physicochemical_properties')].append((bb_idx, phys_index))\n",
    "                edge_data_pos[('building_block', 'has', 'adjacency_matrix')].append((bb_idx, adj_index))\n",
    "                edge_data_pos[('building_block', 'has', 'distance_matrix')].append((bb_idx, dist_index))\n",
    "\n",
    "                edge_data_neg[('building_block', 'has', 'molecular_descriptor')].append((bb_idx, mol_desc_index))\n",
    "                edge_data_neg[('building_block', 'has', 'physicochemical_properties')].append((bb_idx, phys_index))\n",
    "                edge_data_neg[('building_block', 'has', 'adjacency_matrix')].append((bb_idx, adj_index))\n",
    "                edge_data_neg[('building_block', 'has', 'distance_matrix')].append((bb_idx, dist_index))\n",
    "\n",
    "                building_block_index_map[bb_smiles] = bb_idx\n",
    "            else:\n",
    "                bb_idx = building_block_index_map[bb_smiles]\n",
    "\n",
    "            # Add 'contains' edge from small molecule to building block\n",
    "            edge_data_pos[('small_molecule', 'contains', 'building_block')].append((mol_idx, bb_idx))\n",
    "            edge_data_neg[('small_molecule', 'contains', 'building_block')].append((mol_idx, bb_idx))\n",
    "\n",
    "        # Add 'binds' edge from small molecule to protein\n",
    "        protein_idx = protein_index_map[binds_protein]\n",
    "        edge_data_pos[('small_molecule', 'binds', 'protein')].append((mol_idx, protein_idx))\n",
    "\n",
    "        protein_idx = protein_index_map[not_binds_protein]\n",
    "        edge_data_neg[('small_molecule', 'binds', 'protein')].append((mol_idx, protein_idx))\n",
    "\n",
    "    print('Completed processing samples')\n",
    "\n",
    "    # Create graphs\n",
    "    g_pos = dgl.heterograph(edge_data_pos)\n",
    "    g_neg = dgl.heterograph(edge_data_neg)\n",
    "\n",
    "    # Find the maximum feature shape for each node type\n",
    "    max_feature_shapes = {}\n",
    "    for ntype, features in tqdm(node_data.items(), total=len(node_data.items())):\n",
    "        max_shape = torch.Size([0])\n",
    "        for _, feat in features:\n",
    "            if feat['feature'].shape > max_shape:\n",
    "                max_shape = feat['feature'].shape\n",
    "        max_feature_shapes[ntype] = max_shape\n",
    "    \n",
    "    # Function to pad features to the maximum shape\n",
    "    def pad_feature(feature, max_shape):\n",
    "        padding = [0] * (2 * len(max_shape))\n",
    "        for i in range(len(max_shape)):\n",
    "            padding[2 * i + 1] = max_shape[i] - feature.shape[i]\n",
    "        return pad(feature, padding)\n",
    "\n",
    "    # Assign padded features to nodes\n",
    "    for ntype, features in tqdm(node_data.items(), total=len(node_data.items())):\n",
    "        indices, feats = zip(*features)\n",
    "        max_shape = max_feature_shapes[ntype]\n",
    "        padded_feats = [pad_feature(feat['feature'], max_shape) for feat in feats]\n",
    "        g_pos.nodes[ntype].data['feature'] = torch.stack(padded_feats)\n",
    "        g_neg.nodes[ntype].data['feature'] = torch.stack(padded_feats)\n",
    "\n",
    "    # Save the graphs\n",
    "    dgl.save_graphs(f\"./heterogeneous_graph_pos_{variant}.dgl\", [g_pos])\n",
    "    dgl.save_graphs(f\"./heterogeneous_graph_neg_{variant}.dgl\", [g_neg])\n",
    "\n",
    "    # Save building block and protein indices to Parquet\n",
    "    building_block_df = pd.DataFrame.from_dict(building_block_index_map, orient='index', columns=['smiles'])\n",
    "    protein_df = pd.DataFrame.from_dict(protein_index_map, orient='index', columns=['protein_name'])\n",
    "\n",
    "    building_block_df.to_parquet(f\"./building_block_indices_{variant}.parquet\")\n",
    "    protein_df.to_parquet(f\"./protein_indices_{variant}.parquet\")\n",
    "\n",
    "    return g_pos, g_neg\n",
    "\n",
    "def load_heterogeneous_graphs():\n",
    "    g_pos, _ = dgl.load_graphs(\"./heterogeneous_graph_pos.dgl\")\n",
    "    g_neg, _ = dgl.load_graphs(\"./heterogeneous_graph_neg.dgl\")\n",
    "\n",
    "    # Load building block and protein indices\n",
    "    building_block_df = pd.read_parquet(\"./building_block_indices.parquet\")\n",
    "    protein_df = pd.read_parquet(\"./protein_indices.parquet\")\n",
    "\n",
    "    building_block_index_map = building_block_df.to_dict(orient='index')\n",
    "    protein_index_map = protein_df.to_dict(orient='index')\n",
    "\n",
    "    return g_pos[0], g_neg[0], building_block_index_map, protein_index_map\n",
    "\n",
    "# Usage:\n",
    "# File paths\n",
    "train_file_path = 'train_data.parquet'\n",
    "test_file_path = 'test_data.parquet'\n",
    "val_file_path = 'val_data.parquet'\n",
    "protein_files = {\n",
    "    'BRD4': './BRD4.pdb',\n",
    "    'HSA': './ALB.pdb',\n",
    "    'sEH': './EPH.pdb'\n",
    "}\n",
    "\n",
    "# train_pos_g, train_neg_g = create_heterogeneous_graphs(train_file_path, protein_files, 'train')\n",
    "# test_pos_g, test_neg_g = create_heterogeneous_graphs(test_file_path, protein_files, 'test')\n",
    "val_pos_g, val_neg_g = create_heterogeneous_graphs(val_file_path, protein_files, 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc25fc-04b3-40b5-9b7d-ceb09757bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.functional import edge_softmax\n",
    "\n",
    "\n",
    "class HGTLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        node_dict,\n",
    "        edge_dict,\n",
    "        n_heads,\n",
    "        dropout=0.2,\n",
    "        use_norm=False,\n",
    "    ):\n",
    "        super(HGTLayer, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.node_dict = node_dict\n",
    "        self.edge_dict = edge_dict\n",
    "        self.num_types = len(node_dict)\n",
    "        self.num_relations = len(edge_dict)\n",
    "        self.total_rel = self.num_types * self.num_relations * self.num_types\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = out_dim // n_heads\n",
    "        self.sqrt_dk = math.sqrt(self.d_k)\n",
    "        self.att = None\n",
    "\n",
    "        self.k_linears = nn.ModuleList()\n",
    "        self.q_linears = nn.ModuleList()\n",
    "        self.v_linears = nn.ModuleList()\n",
    "        self.a_linears = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.use_norm = use_norm\n",
    "\n",
    "        for t in range(self.num_types):\n",
    "            self.k_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.q_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.v_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.a_linears.append(nn.Linear(out_dim, out_dim))\n",
    "            if use_norm:\n",
    "                self.norms.append(nn.LayerNorm(out_dim))\n",
    "\n",
    "        self.relation_pri = nn.Parameter(\n",
    "            torch.ones(self.num_relations, self.n_heads)\n",
    "        )\n",
    "        self.relation_att = nn.Parameter(\n",
    "            torch.Tensor(self.num_relations, n_heads, self.d_k, self.d_k)\n",
    "        )\n",
    "        self.relation_msg = nn.Parameter(\n",
    "            torch.Tensor(self.num_relations, n_heads, self.d_k, self.d_k)\n",
    "        )\n",
    "        self.skip = nn.Parameter(torch.ones(self.num_types))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_att)\n",
    "        nn.init.xavier_uniform_(self.relation_msg)\n",
    "\n",
    "    def forward(self, G, h):\n",
    "        with G.local_scope():\n",
    "            node_dict, edge_dict = self.node_dict, self.edge_dict\n",
    "            for srctype, etype, dsttype in G.canonical_etypes:\n",
    "                sub_graph = G[srctype, etype, dsttype]\n",
    "\n",
    "                k_linear = self.k_linears[node_dict[srctype]]\n",
    "                v_linear = self.v_linears[node_dict[srctype]]\n",
    "                q_linear = self.q_linears[node_dict[dsttype]]\n",
    "\n",
    "                k = k_linear(h[srctype]).view(-1, self.n_heads, self.d_k)\n",
    "                v = v_linear(h[srctype]).view(-1, self.n_heads, self.d_k)\n",
    "                q = q_linear(h[dsttype]).view(-1, self.n_heads, self.d_k)\n",
    "\n",
    "                e_id = self.edge_dict[etype]\n",
    "\n",
    "                relation_att = self.relation_att[e_id]\n",
    "                relation_pri = self.relation_pri[e_id]\n",
    "                relation_msg = self.relation_msg[e_id]\n",
    "\n",
    "                k = torch.einsum(\"bij,ijk->bik\", k, relation_att)\n",
    "                v = torch.einsum(\"bij,ijk->bik\", v, relation_msg)\n",
    "\n",
    "                sub_graph.srcdata[\"k\"] = k\n",
    "                sub_graph.dstdata[\"q\"] = q\n",
    "                sub_graph.srcdata[\"v_%d\" % e_id] = v\n",
    "\n",
    "                sub_graph.apply_edges(fn.v_dot_u(\"q\", \"k\", \"t\"))\n",
    "                attn_score = (\n",
    "                    sub_graph.edata.pop(\"t\").sum(-1)\n",
    "                    * relation_pri\n",
    "                    / self.sqrt_dk\n",
    "                )\n",
    "                attn_score = edge_softmax(sub_graph, attn_score, norm_by=\"dst\")\n",
    "\n",
    "                sub_graph.edata[\"t\"] = attn_score.unsqueeze(-1)\n",
    "\n",
    "            G.multi_update_all(\n",
    "                {\n",
    "                    etype: (\n",
    "                        fn.u_mul_e(\"v_%d\" % e_id, \"t\", \"m\"),\n",
    "                        fn.sum(\"m\", \"t\"),\n",
    "                    )\n",
    "                    for etype, e_id in edge_dict.items()\n",
    "                },\n",
    "                cross_reducer=\"mean\",\n",
    "            )\n",
    "\n",
    "            new_h = {}\n",
    "            for ntype in G.ntypes:\n",
    "                \"\"\"\n",
    "                Step 3: Target-specific Aggregation\n",
    "                x = norm( W[node_type] * gelu( Agg(x) ) + x )\n",
    "                \"\"\"\n",
    "                n_id = node_dict[ntype]\n",
    "                alpha = torch.sigmoid(self.skip[n_id])\n",
    "                t = G.nodes[ntype].data[\"t\"].view(-1, self.out_dim)\n",
    "                trans_out = self.drop(self.a_linears[n_id](t))\n",
    "                trans_out = trans_out * alpha + h[ntype] * (1 - alpha)\n",
    "                if self.use_norm:\n",
    "                    new_h[ntype] = self.norms[n_id](trans_out)\n",
    "                else:\n",
    "                    new_h[ntype] = trans_out\n",
    "            return new_h\n",
    "\n",
    "class HeteroDotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h, etype):\n",
    "        with graph.local_scope():\n",
    "            for ntype in graph.ntypes:\n",
    "                graph[ntype].data['h'] = h[ntype]\n",
    "\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'), etype=etype)\n",
    "            return graph.edges[etype].data['score']\n",
    "\n",
    "\n",
    "class HGT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_dict,\n",
    "        edge_dict,\n",
    "        in_dim_dict,\n",
    "        n_hid,\n",
    "        n_out,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        use_norm=True,\n",
    "    ):\n",
    "        super(HGT, self).__init__()\n",
    "        self.node_dict = node_dict\n",
    "        self.edge_dict = edge_dict\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.n_hid = n_hid\n",
    "        self.n_out = n_out\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.adapt_ws = nn.ModuleList()\n",
    "        for ntype, in_dim in in_dim_dict.items():\n",
    "            if len(in_dim) == 1:\n",
    "                self.adapt_ws[ntype] = nn.Linear(in_dim[0], n_hid)\n",
    "            elif len(in_dim) == 2:\n",
    "                self.adapt_ws[ntype] = nn.Sequential(\n",
    "                    nn.Conv1d(in_dim[0], n_hid, kernel_size=1),\n",
    "                    nn.Conv1d(n_hid, n_hid, kernel_size=1),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(n_hid * in_dim[1], n_hid)\n",
    "                )\n",
    "            elif len(in_dim) == 3:\n",
    "                self.adapt_ws[ntype] = nn.Sequential(\n",
    "                    nn.Conv2d(in_dim[0], n_hid, kernel_size=1),\n",
    "                    nn.Conv2d(n_hid, n_hid, kernel_size=1),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(n_hid * in_dim[1] * in_dim[2], n_hid)\n",
    "                )\n",
    "            elif len(in_dim) == 4:\n",
    "                self.adapt_ws[ntype] = nn.Sequential(\n",
    "                    nn.Conv3d(in_dim[0], n_hid, kernel_size=1),\n",
    "                    nn.Conv3d(n_hid, n_hid, kernel_size=1),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(n_hid * in_dim[1] * in_dim[2] * in_dim[3], n_hid)\n",
    "                )\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            self.gcs.append(\n",
    "                HGTLayer(\n",
    "                    n_hid,\n",
    "                    n_hid,\n",
    "                    node_dict,\n",
    "                    edge_dict,\n",
    "                    n_heads,\n",
    "                    use_norm=use_norm,\n",
    "                )\n",
    "            )\n",
    "        self.pred = HeteroDotProductPredictor()\n",
    "\n",
    "    def forward(self, G, neg_g, etype):\n",
    "        h = {}\n",
    "        for ntype in G.ntypes:\n",
    "            n_id = self.node_dict[ntype]\n",
    "            h[ntype] = F.gelu(self.adapt_ws[n_id](G.nodes[ntype].data[\"feature\"]))\n",
    "        for i in range(self.n_layers):\n",
    "            h = self.gcs[i](G, h)\n",
    "        return self.pred(g, h, etype), self.pred(neg_g, h, etype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a428dd2-1000-4ba6-b927-c9cdaf364f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import EdgeDataLoader\n",
    "from dgl.sampling import global_uniform_negative_sampling\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    n_edges = pos_score.shape[0]\n",
    "    return (1 - pos_score + neg_score.view(n_edges, -1)).clamp(min=0).mean()\n",
    "\n",
    "def train(model, pos_g, neg_g, edge_type, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        pos_score, neg_score = model(pos_g, neg_g, edge_type)\n",
    "        loss = compute_loss(pos_score, neg_score)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}')\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "node_dict = {}\n",
    "edge_dict = {}\n",
    "in_dim_dict = {ntype: pos_g.nodes[ntype].data['feature'].shape[1] for ntype in pos_g.ntypes}\n",
    "for ntype in pos_g.ntypes:\n",
    "    node_dict[ntype] = len(node_dict)\n",
    "for etype in pos_g.etypes:\n",
    "    edge_dict[etype] = len(edge_dict)\n",
    "    pos_g.edges[etype].data[\"id\"] = (\n",
    "        torch.ones(pos_g.num_edges(etype), dtype=torch.long) * edge_dict[etype]\n",
    "    )\n",
    "\n",
    "model = HGT(\n",
    "    node_dict=node_dict,\n",
    "    edge_dict=edge_dict,\n",
    "    in_dim_dict = in_dim_dict,\n",
    "    n_hid=128,\n",
    "    n_out=32,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    "    use_norm=True\n",
    ")\n",
    "epochs = 100\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, total_steps = epochs, max_lr = 1e-3\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "train(model, pos_g, neg_g, 'binds', optimizer, scheduler, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d100de4-ded8-4c1f-ac71-4abaa745287e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
