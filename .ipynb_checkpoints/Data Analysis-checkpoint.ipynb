{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b952cb-00a1-4e51-bbd5-02fcc0baaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('train.parquet')\n",
    "print(parquet_file.schema.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5e258-03e7-4745-b09d-783c1ed20fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e723c9-0aed-4669-89a4-a1f45bcf0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to input and output Parquet files\n",
    "input_file = 'train.parquet'\n",
    "output_file = 'filtered_train.parquet'\n",
    "\n",
    "# Remove output file if it exists\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Open the Parquet file\n",
    "pf = pq.ParquetFile(input_file)\n",
    "\n",
    "# Get total number of row groups (batches)\n",
    "total_row_groups = pf.num_row_groups\n",
    "\n",
    "# First Pass: Build mapping of molecule_smiles to the set of proteins it binds to\n",
    "print(\"First Pass: Building molecule to proteins mapping...\")\n",
    "\n",
    "# Initialize a dictionary to store mappings\n",
    "molecule_binds = defaultdict(set)\n",
    "all_proteins = set()\n",
    "\n",
    "for rg in tqdm(range(total_row_groups), desc=\"Processing Batches\"):\n",
    "    # Read a row group with necessary columns\n",
    "    batch = pf.read_row_group(rg, columns=['molecule_smiles', 'protein_name', 'binds'])\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Update all_proteins set\n",
    "    all_proteins.update(df['protein_name'].unique())\n",
    "    \n",
    "    # Filter rows where 'binds' == 1\n",
    "    df_binds_1 = df[df['binds'] == 1]\n",
    "    \n",
    "    # Update molecule_binds mapping\n",
    "    for idx, row in df_binds_1.iterrows():\n",
    "        molecule = row['molecule_smiles']\n",
    "        protein = row['protein_name']\n",
    "        molecule_binds[molecule].add(protein)\n",
    "    \n",
    "    # Clear variables to free memory\n",
    "    del df, df_binds_1, batch\n",
    "\n",
    "# Convert all_proteins to a list\n",
    "all_proteins = list(all_proteins)\n",
    "\n",
    "# Second Pass: Process data and write to output\n",
    "print(\"Second Pass: Filtering dataset and writing to new Parquet file...\")\n",
    "\n",
    "# Initialize Parquet writer\n",
    "writer = None\n",
    "\n",
    "for rg in tqdm(range(total_row_groups), desc=\"Processing Batches\"):\n",
    "    # Read the row group\n",
    "    batch = pf.read_row_group(rg)\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Filter molecules that have at least one binds == 1\n",
    "    df = df[df['molecule_smiles'].isin(molecule_binds.keys())]\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Prepare to select rows to include\n",
    "        rows_to_include = []\n",
    "        \n",
    "        # Process each molecule in the batch\n",
    "        for molecule, group in df.groupby('molecule_smiles'):\n",
    "            binds_1_proteins = molecule_binds[molecule]\n",
    "            num_binds_1 = len(binds_1_proteins)\n",
    "            \n",
    "            if num_binds_1 > 1:\n",
    "                # Include all rows for this molecule\n",
    "                rows_to_include.append(group)\n",
    "            elif num_binds_1 == 1:\n",
    "                # Include the positive binding row\n",
    "                positive_row = group[group['binds'] == 1]\n",
    "                \n",
    "                # Include one negative binding row\n",
    "                unbound_proteins = set(all_proteins) - binds_1_proteins\n",
    "                # Select one unbound protein\n",
    "                unbound_protein = unbound_proteins.pop()\n",
    "                negative_row = group[(group['protein_name'] == unbound_protein) & (group['binds'] == 0)]\n",
    "                \n",
    "                # If negative_row is empty, skp it\n",
    "                if  not negative_row.empty:\n",
    "                    # Append the positive and negative rows\n",
    "                    rows_to_include.append(pd.concat([positive_row, negative_row]))\n",
    "\n",
    "        \n",
    "        if rows_to_include:\n",
    "            filtered_df = pd.concat(rows_to_include)\n",
    "            \n",
    "            # Convert to PyArrow Table\n",
    "            table = pa.Table.from_pandas(filtered_df)\n",
    "            \n",
    "            # Initialize the Parquet writer if not already done\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(output_file, table.schema)\n",
    "            \n",
    "            # Write the table to the Parquet file\n",
    "            writer.write_table(table)\n",
    "            \n",
    "            # Clear variables to free memory\n",
    "            del table, filtered_df\n",
    "    \n",
    "    # Clear variables to free memory\n",
    "    del df, batch\n",
    "\n",
    "# Close the Parquet writer\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "print(\"Filtering completed. Filtered dataset saved to 'filtered_train.parquet'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978c650-5392-4775-80c8-cf190519c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('filtered_train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f998fa-441c-4f0d-8d81-158d50b3831e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x3129fe670>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, DataLoader, Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441e2308-dc4e-49ad-b3db-a0e5cdfe5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of 'binds' in the dataset:\n",
      "binds\n",
      "1    1589903\n",
      "0    1509717\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Assuming the dataset is in a Parquet file named 'filtered_train.parquet'\n",
    "df = pd.read_parquet('filtered_train.parquet')\n",
    "\n",
    "# Check the distribution of 'binds'\n",
    "print(\"Distribution of 'binds' in the dataset:\")\n",
    "print(df['binds'].value_counts())\n",
    "\n",
    "# Stratified splitting\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['binds'],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f3ad4c8-711f-49cb-a28d-a5a9ce2d7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, HeteroData, DataLoader\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.Polypeptide import three_to_index, index_to_one\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set random seed and device\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba3d7b1-2dbc-475b-8976-ced84be1d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, Linear, global_mean_pool\n",
    "from torch.utils.data import random_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.Polypeptide import three_to_index, index_to_one\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Helper functions\n",
    "def residue_name_to_idx(res_name_one):\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G',\n",
    "                   'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S',\n",
    "                   'T', 'W', 'Y', 'V']\n",
    "    if res_name_one in amino_acids:\n",
    "        return amino_acids.index(res_name_one)\n",
    "    else:\n",
    "        return len(amino_acids)  # Unknown amino acid\n",
    "\n",
    "def process_protein(pdb_file, threshold=5.0):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('protein', pdb_file)\n",
    "\n",
    "    amino_acids = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if is_aa(residue):\n",
    "                    amino_acids.append(residue)\n",
    "\n",
    "    amino_acid_types = [index_to_one(three_to_index(residue.get_resname())) for residue in amino_acids]\n",
    "    unique_amino_acids = list(set(amino_acid_types))\n",
    "\n",
    "    data = HeteroData()\n",
    "\n",
    "    node_features = {}\n",
    "    node_positions = {}\n",
    "    node_counter = 0\n",
    "\n",
    "    # Initialize node features and positions\n",
    "    for aa_type in unique_amino_acids:\n",
    "        node_features[aa_type] = []\n",
    "        node_positions[aa_type] = []\n",
    "\n",
    "    for idx, (residue, aa_type) in enumerate(zip(amino_acids, amino_acid_types)):\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            pos = ca_atom.get_coord()\n",
    "        except KeyError:\n",
    "            pos = [0.0, 0.0, 0.0]\n",
    "        node_features[aa_type].append([residue_name_to_idx(aa_type)])\n",
    "        node_positions[aa_type].append(pos)\n",
    "        node_counter += 1\n",
    "\n",
    "    for aa_type in unique_amino_acids:\n",
    "        data[aa_type].x = torch.tensor(node_features[aa_type], dtype=torch.float)\n",
    "        data[aa_type].pos = torch.tensor(np.array(node_positions[aa_type]), dtype=torch.float)\n",
    "\n",
    "    # Build edges based on proximity\n",
    "    contact_edge_index = {}\n",
    "    edge_types = set()\n",
    "    reverse_edge_types = set()\n",
    "\n",
    "    # Mapping from global index to local index within node type\n",
    "    global_to_local_idx = {}\n",
    "    current_idx = {aa_type: 0 for aa_type in unique_amino_acids}\n",
    "\n",
    "    for aa_type in amino_acid_types:\n",
    "        global_to_local_idx[aa_type] = {}\n",
    "\n",
    "    for idx, aa_type in enumerate(amino_acid_types):\n",
    "        global_idx = idx\n",
    "        local_idx = current_idx[aa_type]\n",
    "        global_to_local_idx[aa_type][global_idx] = local_idx\n",
    "        current_idx[aa_type] += 1\n",
    "\n",
    "    num_residues = len(amino_acids)\n",
    "    for i in range(num_residues):\n",
    "        residue_i = amino_acids[i]\n",
    "        aa_i = amino_acid_types[i]\n",
    "        try:\n",
    "            ca_i = residue_i['CA']\n",
    "            pos_i = ca_i.get_coord()\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j in range(i + 1, num_residues):  # Ensure j > i to avoid duplicates\n",
    "            residue_j = amino_acids[j]\n",
    "            aa_j = amino_acid_types[j]\n",
    "            try:\n",
    "                ca_j = residue_j['CA']\n",
    "                pos_j = ca_j.get_coord()\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            distance = np.linalg.norm(pos_i - pos_j)\n",
    "            if distance <= threshold:\n",
    "                # Define edge type in consistent order\n",
    "                if aa_i <= aa_j:\n",
    "                    edge_type = (aa_i, 'contact', aa_j)\n",
    "                    src_aa, tgt_aa = aa_i, aa_j\n",
    "                    src_global, tgt_global = i, j\n",
    "                else:\n",
    "                    edge_type = (aa_j, 'contact', aa_i)\n",
    "                    src_aa, tgt_aa = aa_j, aa_i\n",
    "                    src_global, tgt_global = j, i\n",
    "\n",
    "                # Initialize edge list if not present\n",
    "                if edge_type not in contact_edge_index:\n",
    "                    contact_edge_index[edge_type] = []\n",
    "\n",
    "                # Get local indices within their respective node types\n",
    "                src_local = global_to_local_idx[src_aa][src_global]\n",
    "                tgt_local = global_to_local_idx[tgt_aa][tgt_global]\n",
    "\n",
    "                # Append edge\n",
    "                contact_edge_index[edge_type].append([src_local, tgt_local])\n",
    "                edge_types.add(edge_type)\n",
    "\n",
    "    # Assign edges to HeteroData\n",
    "    for edge_type, edges in contact_edge_index.items():\n",
    "        if len(edges) > 0:\n",
    "            # Extract the original source and target types\n",
    "            src_type, relation, tgt_type = edge_type\n",
    "    \n",
    "            # Create reverse edge type\n",
    "            reverse_edge_type = (tgt_type, relation, src_type)\n",
    "            reverse_edge_types.add(reverse_edge_type)\n",
    "    \n",
    "            # Convert original edges to tensor\n",
    "            edge_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "            # Assign original edges to original edge type\n",
    "            data[edge_type].edge_index = edge_tensor\n",
    "    \n",
    "            # Create reverse edges\n",
    "            reverse_edges = [[tgt, src] for src, tgt in edges]\n",
    "            reverse_edge_tensor = torch.tensor(reverse_edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "            # Assign reverse edges to reverse edge type\n",
    "            data[reverse_edge_type].edge_index = reverse_edge_tensor\n",
    "\n",
    "    data.node_types = set(unique_amino_acids)\n",
    "    data.edge_types = edge_types\n",
    "    data.reverse_edge_types = reverse_edge_types\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13642fd5-cca3-4e39-a5f3-32d7a5c15906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "def process_row(row):\n",
    "    smiles = row['molecule_smiles']\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return set(), set()\n",
    "\n",
    "    atoms_to_remove = [atom.GetIdx() for atom in mol.GetAtoms() if atom.GetSymbol() == 'Dy']\n",
    "    mol = Chem.EditableMol(mol)\n",
    "    for idx in sorted(atoms_to_remove, reverse=True):\n",
    "        mol.RemoveAtom(idx)\n",
    "\n",
    "    mol = mol.GetMol()\n",
    "\n",
    "    mol = Chem.AddHs(mol)\n",
    "    atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]  # Keep this as a list\n",
    "    unique_atom_types = set(atom_types)  # Use set to get unique atom types\n",
    "    edge_types = set()\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        atype_i = atom_types[i]\n",
    "        atype_j = atom_types[j]\n",
    "        edge_types.add((atype_i, 'bond', atype_j))\n",
    "        edge_types.add((atype_j, 'bond', atype_i))\n",
    "\n",
    "    return unique_atom_types, edge_types\n",
    "\n",
    "    return atom_types, edge_types\n",
    "\n",
    "def collect_molecule_node_and_edge_types(df):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_row)(row) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting molecule types\")\n",
    "    )\n",
    "    molecule_node_types = set()\n",
    "    molecule_edge_types = set()\n",
    "    for node_types, edge_types in results:\n",
    "        molecule_node_types.update(node_types)\n",
    "        molecule_edge_types.update(edge_types)\n",
    "    return sorted(molecule_node_types), sorted(molecule_edge_types)\n",
    "\n",
    "def collect_protein_node_and_edge_types(protein_graphs):\n",
    "    protein_node_types = set()\n",
    "    protein_edge_types = set()\n",
    "    for protein_data in protein_graphs.values():\n",
    "        protein_node_types.update(protein_data.node_types)\n",
    "        protein_edge_types.update(protein_data.edge_types)\n",
    "        protein_edge_types.update(protein_data.reverse_edge_types)\n",
    "    return sorted(protein_node_types), sorted(protein_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de232ea6-bc50-431f-80b6-7039be7b0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# def collect_molecule_node_and_edge_types(df):\n",
    "#     molecule_node_types = set()\n",
    "#     molecule_edge_types = set()\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting molecule types\"):\n",
    "#         smiles = row['molecule_smiles']\n",
    "#         mol = Chem.MolFromSmiles(smiles)\n",
    "#         if mol is None:\n",
    "#             continue\n",
    "\n",
    "#         atoms_to_remove = [atom.GetIdx() for atom in mol.GetAtoms() if atom.GetSymbol() == 'Dy']\n",
    "#         mol = Chem.EditableMol(mol)\n",
    "#         for idx in sorted(atoms_to_remove, reverse=True):\n",
    "#             mol.RemoveAtom(idx)\n",
    "\n",
    "#         mol = mol.GetMol()\n",
    "\n",
    "#         mol = Chem.AddHs(mol)\n",
    "#         atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "#         molecule_node_types.update(atom_types)\n",
    "#         for bond in mol.GetBonds():\n",
    "#             i = bond.GetBeginAtomIdx()\n",
    "#             j = bond.GetEndAtomIdx()\n",
    "#             atype_i = atom_types[i]\n",
    "#             atype_j = atom_types[j]\n",
    "#             edge_type = (atype_i, 'bond', atype_j)\n",
    "#             molecule_edge_types.add(edge_type)\n",
    "#             edge_type = (atype_j, 'bond', atype_i)\n",
    "#             molecule_edge_types.add(edge_type)\n",
    "#     return sorted(molecule_node_types), sorted(molecule_edge_types)\n",
    "\n",
    "# def collect_protein_node_and_edge_types(protein_graphs):\n",
    "#     protein_node_types = set()\n",
    "#     protein_edge_types = set()\n",
    "#     for protein_data in protein_graphs.values():\n",
    "#         protein_node_types.update(protein_data.node_types)\n",
    "#         protein_edge_types.update(protein_data.edge_types)\n",
    "#         protein_edge_types.update(protein_data.reverse_edge_types)\n",
    "#     return sorted(protein_node_types), sorted(protein_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe0fa18-44c9-4417-9c01-08f52c2085ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split completed.\n",
      "Training set size: 2169734\n",
      "Validation set size: 464943\n",
      "Test set size: 464943\n",
      "Collecting molecule node and edge types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting molecule types:   0%|        | 14/3099620 [00:00<17:55:07, 48.05it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/kushaldsouza/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/Users/kushaldsouza/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kushaldsouza/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/vl/nn1r9fg115dd94js8h25w1yc0000gn/T/ipykernel_1683/1359746264.py\", line 24, in process_row\nTypeError: 'set' object is not subscriptable\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Collect node and edge types with progress tracking\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting molecule node and edge types...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m molecule_node_types, molecule_edge_types \u001b[38;5;241m=\u001b[39m collect_molecule_node_and_edge_types(df)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting protein node and edge types...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m protein_node_types, protein_edge_types \u001b[38;5;241m=\u001b[39m collect_protein_node_and_edge_types(protein_graphs)\n",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mcollect_molecule_node_and_edge_types\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_molecule_node_and_edge_types\u001b[39m(df):\n\u001b[0;32m---> 32\u001b[0m     results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(\n\u001b[1;32m     33\u001b[0m         delayed(process_row)(row) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting molecule types\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m     molecule_node_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     36\u001b[0m     molecule_edge_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_parquet('filtered_train.parquet')\n",
    "\n",
    "# Stratified splitting\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nSplit completed.\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Process and store protein graphs\n",
    "protein_graphs = {}\n",
    "protein_pdb_files = {\n",
    "    'BRD4': './BRD4.pdb',\n",
    "    'HSA': './ALB.pdb',\n",
    "    'sEH': './EPH.pdb'\n",
    "}\n",
    "\n",
    "for protein_name, pdb_file in protein_pdb_files.items():\n",
    "    if os.path.exists(pdb_file):\n",
    "        protein_data = process_protein(pdb_file)\n",
    "        protein_graphs[protein_name] = protein_data\n",
    "    else:\n",
    "        print(f\"PDB file {pdb_file} for {protein_name} does not exist.\")\n",
    "\n",
    "# Collect node and edge types with progress tracking\n",
    "print(\"Collecting molecule node and edge types...\")\n",
    "molecule_node_types, molecule_edge_types = collect_molecule_node_and_edge_types(df)\n",
    "\n",
    "print(\"Collecting protein node and edge types...\")\n",
    "protein_node_types, protein_edge_types = collect_protein_node_and_edge_types(protein_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd96d2-231d-4b13-a6ca-6243fd5c2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=4):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_K = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_V = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.scale = self.head_dim ** 0.5\n",
    "\n",
    "    def forward(self, query_nodes, key_nodes):\n",
    "        # query_nodes: [N_q, hidden_dim]\n",
    "        # key_nodes: [N_k, hidden_dim]\n",
    "\n",
    "        N_q = query_nodes.size(0)\n",
    "        N_k = key_nodes.size(0)\n",
    "\n",
    "        Q = self.W_Q(query_nodes)  # [N_q, hidden_dim]\n",
    "        K = self.W_K(key_nodes)    # [N_k, hidden_dim]\n",
    "        V = self.W_V(key_nodes)    # [N_k, hidden_dim]\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(N_q, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_q, head_dim]\n",
    "        K = K.view(N_k, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_k, head_dim]\n",
    "        V = V.view(N_k, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_k, head_dim]\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [num_heads, N_q, N_k]\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)                # [num_heads, N_q, N_k]\n",
    "\n",
    "        # Compute attended values\n",
    "        out = torch.matmul(attn_weights, V)  # [num_heads, N_q, head_dim]\n",
    "        out = out.permute(1, 0, 2).contiguous().view(N_q, -1)  # [N_q, hidden_dim]\n",
    "\n",
    "        return out\n",
    "\n",
    "class CrossGraphAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_attention_heads=4):\n",
    "        super(CrossGraphAttentionModel, self).__init__()\n",
    "\n",
    "        # print(molecule_edge_types)\n",
    "\n",
    "        # Molecule GNN Encoder\n",
    "        self.mol_conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in molecule_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.mol_conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in molecule_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Protein GNN Encoder\n",
    "        self.prot_conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in protein_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.prot_conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in protein_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Cross-Attention Layers\n",
    "        self.cross_attn_mol_to_prot = CrossAttentionLayer(hidden_dim, num_attention_heads)\n",
    "        self.cross_attn_prot_to_mol = CrossAttentionLayer(hidden_dim, num_attention_heads)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, mol_data, prot_data):\n",
    "        # Molecule GNN Encoding\n",
    "        x_mol_dict = mol_data.x_dict\n",
    "        edge_index_mol_dict = mol_data.edge_index_dict\n",
    "\n",
    "        x_mol_dict = self.mol_conv1(x_mol_dict, edge_index_mol_dict)\n",
    "        x_mol_dict = {key: F.relu(x) for key, x in x_mol_dict.items()}\n",
    "\n",
    "        x_mol_dict = self.mol_conv2(x_mol_dict, edge_index_mol_dict)\n",
    "        x_mol_dict = {key: F.relu(x) for key, x in x_mol_dict.items()}\n",
    "\n",
    "        # Concatenate molecule node embeddings\n",
    "        mol_node_embeddings = []\n",
    "        for nt in molecule_node_types:\n",
    "            if nt in x_mol_dict:\n",
    "                mol_node_embeddings.append(x_mol_dict[nt])\n",
    "        H_mol = torch.cat(mol_node_embeddings, dim=0)\n",
    "\n",
    "        # Protein GNN Encoding\n",
    "        x_prot_dict = prot_data.x_dict\n",
    "        edge_index_prot_dict = prot_data.edge_index_dict\n",
    "\n",
    "        x_prot_dict = self.prot_conv1(x_prot_dict, edge_index_prot_dict)\n",
    "        x_prot_dict = {key: F.relu(x) for key, x in x_prot_dict.items()}\n",
    "\n",
    "        x_prot_dict = self.prot_conv2(x_prot_dict, edge_index_prot_dict)\n",
    "        x_prot_dict = {key: F.relu(x) for key, x in x_prot_dict.items()}\n",
    "\n",
    "        # Concatenate protein node embeddings\n",
    "        prot_node_embeddings = []\n",
    "        for nt in protein_node_types:\n",
    "            if nt in x_prot_dict:\n",
    "                prot_node_embeddings.append(x_prot_dict[nt])\n",
    "        H_prot = torch.cat(prot_node_embeddings, dim=0)\n",
    "\n",
    "        # Cross-Attention\n",
    "        H_mol_attn = self.cross_attn_mol_to_prot(H_mol, H_prot)\n",
    "        H_prot_attn = self.cross_attn_prot_to_mol(H_prot, H_mol)\n",
    "\n",
    "        # Combine original and attended embeddings\n",
    "        H_mol_combined = H_mol + H_mol_attn\n",
    "        H_prot_combined = H_prot + H_prot_attn\n",
    "\n",
    "        # # Global Pooling\n",
    "        # mol_batch = mol_data.batch if hasattr(mol_data, 'batch') else torch.zeros(H_mol_combined.size(0), dtype=torch.long, device=H_mol_combined.device)\n",
    "        # prot_batch = prot_data.batch if hasattr(prot_data, 'batch') else torch.zeros(H_prot_combined.size(0), dtype=torch.long, device=H_prot_combined.device)\n",
    "\n",
    "        # Global Pooling\n",
    "        # Use batch_dict to get batch information per node type\n",
    "        mol_batches = torch.cat([mol_data.batch_dict[nt] for nt in molecule_node_types if nt in mol_data.batch_dict])\n",
    "        prot_batches = torch.cat([prot_data.batch_dict[nt] for nt in protein_node_types if nt in prot_data.batch_dict])\n",
    "\n",
    "        # z_mol = global_mean_pool(H_mol_combined, mol_batch)\n",
    "        # z_prot = global_mean_pool(H_prot_combined, prot_batch)\n",
    "\n",
    "        z_mol = global_mean_pool(H_mol_combined, mol_batches)\n",
    "        z_prot = global_mean_pool(H_prot_combined, prot_batches)\n",
    "\n",
    "        # Joint Representation\n",
    "        z_joint = torch.cat([z_mol, z_prot], dim=1)\n",
    "\n",
    "        # Prediction\n",
    "        x = F.relu(self.fc1(z_joint))\n",
    "        out = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6327f-d5cd-4fed-9471-f6db72306c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import CombinedDataset, MoleculeDataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CombinedDataset(train_df, protein_graphs)\n",
    "val_dataset = CombinedDataset(val_df, protein_graphs)\n",
    "test_dataset = CombinedDataset(test_df, protein_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99aeef-8131-49da-9d14-2c780ee6e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    mol_batch = [item[0] for item in batch if item is not None and item[0] is not None and item[0]['invalid'] is False]\n",
    "    prot_batch = [item[1] for item in batch if item is not None and item[0] is not None and item[0]['invalid'] is False]\n",
    "\n",
    "    mol_batch = Batch.from_data_list(mol_batch)\n",
    "    prot_batch = Batch.from_data_list(prot_batch)\n",
    "\n",
    "    return mol_batch, prot_batch\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = GeoDataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=12, collate_fn=collate_fn)\n",
    "val_loader = GeoDataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=12, collate_fn=collate_fn)\n",
    "test_loader = GeoDataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=12, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a0925-d271-4a4c-b81d-5d184b377569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node_and_edge_info(data):\n",
    "    print(\"\\nNode and Edge Information:\")\n",
    "    \n",
    "    # Iterate over all node types and print their counts\n",
    "    for node_type in data.node_types:\n",
    "        if node_type == 'smolecule':  # Ignore smolecule\n",
    "            continue\n",
    "        \n",
    "        num_nodes = data[node_type].x.size(0)\n",
    "        print(f\"Node type: {node_type}, Number of nodes: {num_nodes}\")\n",
    "    \n",
    "    # Iterate over all edge types and print relevant information\n",
    "    for edge_type, edge_index in data.edge_index_dict.items():\n",
    "        if 'smolecule' in edge_type:  # Ignore any edges involving smolecule\n",
    "            continue\n",
    "        \n",
    "        src_type, _, tgt_type = edge_type\n",
    "        max_src_idx = edge_index[0].max().item()\n",
    "        max_tgt_idx = edge_index[1].max().item()\n",
    "\n",
    "        num_src_nodes = data[src_type].x.size(0)\n",
    "        num_tgt_nodes = data[tgt_type].x.size(0)\n",
    "\n",
    "        print(f\"Edge type: {edge_type}, Edge index shape: {edge_index.shape}\")\n",
    "        print(f\"Max index in edge_index: src = {max_src_idx}, tgt = {max_tgt_idx}\")\n",
    "        print(f\"Num src nodes ({src_type}): {num_src_nodes}, Num tgt nodes ({tgt_type}): {num_tgt_nodes}\")\n",
    "\n",
    "        # Validation check to identify if there are out-of-bound indices\n",
    "        if max_src_idx >= num_src_nodes or max_tgt_idx >= num_tgt_nodes:\n",
    "            print(f\"Warning: Edge indices out of bounds for edge type {edge_type}\")\n",
    "            print(f\"  Max src index: {max_src_idx} (Num src nodes: {num_src_nodes})\")\n",
    "            print(f\"  Max tgt index: {max_tgt_idx} (Num tgt nodes: {num_tgt_nodes})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5baf606-8eea-4ab1-9b47-cab4b60eb110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting molecule types:   0%|        | 14/3099620 [00:20<17:55:07, 48.05it/s]"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"You are using `torch.load` with `weights_only=False`\")\n",
    "\n",
    "hidden_dim = 64\n",
    "num_attention_heads = 4\n",
    "\n",
    "model = CrossGraphAttentionModel(\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_attention_heads=num_attention_heads\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mol_data, prot_data in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        mol_data = mol_data.to(device)\n",
    "        prot_data = prot_data.to(device)\n",
    "        # print_node_and_edge_info(mol_data)\n",
    "        out = model(mol_data, prot_data)\n",
    "        y = mol_data['smolecule'].y.to(device)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(loader, mode='Validation'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mol_data, prot_data in tqdm(loader, desc=mode):\n",
    "            mol_data = mol_data.to(device)\n",
    "            prot_data = prot_data.to(device)\n",
    "            out = model(mol_data, prot_data)\n",
    "            y = mol_data['smolecule'].y.to(device)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss = evaluate(val_loader, mode='Validation')\n",
    "    print(f'Epoch: {epoch:02d}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'cross_graph_attention_model.pth')\n",
    "\n",
    "# Prediction and evaluation on test data\n",
    "def predict(loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for mol_data, prot_data in tqdm(loader, desc=\"Testing\"):\n",
    "            mol_data = mol_data.to(device)\n",
    "            prot_data = prot_data.to(device)\n",
    "            out = model(mol_data, prot_data)\n",
    "            predictions.extend(out.cpu().numpy())\n",
    "            true_labels.extend(mol_data['smolecule'].y.cpu().numpy())\n",
    "    return predictions, true_labels\n",
    "\n",
    "test_predictions, test_true = predict(test_loader)\n",
    "\n",
    "# Apply a threshold to obtain binary predictions\n",
    "threshold = 0.5\n",
    "test_pred_binary = [1 if p >= threshold else 0 for p in test_predictions]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(test_true, test_pred_binary)\n",
    "roc_auc = roc_auc_score(test_true, test_predictions)\n",
    "precision = precision_score(test_true, test_pred_binary)\n",
    "recall = recall_score(test_true, test_pred_binary)\n",
    "f1 = f1_score(test_true, test_pred_binary)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96af9b-fc09-467d-9239-89a793fc87d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
