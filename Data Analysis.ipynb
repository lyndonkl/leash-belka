{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b952cb-00a1-4e51-bbd5-02fcc0baaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('train.parquet')\n",
    "print(parquet_file.schema.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5e258-03e7-4745-b09d-783c1ed20fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e723c9-0aed-4669-89a4-a1f45bcf0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to input and output Parquet files\n",
    "input_file = 'train.parquet'\n",
    "output_file = 'filtered_train.parquet'\n",
    "\n",
    "# Remove output file if it exists\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Open the Parquet file\n",
    "pf = pq.ParquetFile(input_file)\n",
    "\n",
    "# Get total number of row groups (batches)\n",
    "total_row_groups = pf.num_row_groups\n",
    "\n",
    "# First Pass: Build mapping of molecule_smiles to the set of proteins it binds to\n",
    "print(\"First Pass: Building molecule to proteins mapping...\")\n",
    "\n",
    "# Initialize a dictionary to store mappings\n",
    "molecule_binds = defaultdict(set)\n",
    "all_proteins = set()\n",
    "\n",
    "for rg in tqdm(range(total_row_groups), desc=\"Processing Batches\"):\n",
    "    # Read a row group with necessary columns\n",
    "    batch = pf.read_row_group(rg, columns=['molecule_smiles', 'protein_name', 'binds'])\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Update all_proteins set\n",
    "    all_proteins.update(df['protein_name'].unique())\n",
    "    \n",
    "    # Filter rows where 'binds' == 1\n",
    "    df_binds_1 = df[df['binds'] == 1]\n",
    "    \n",
    "    # Update molecule_binds mapping\n",
    "    for idx, row in df_binds_1.iterrows():\n",
    "        molecule = row['molecule_smiles']\n",
    "        protein = row['protein_name']\n",
    "        molecule_binds[molecule].add(protein)\n",
    "    \n",
    "    # Clear variables to free memory\n",
    "    del df, df_binds_1, batch\n",
    "\n",
    "# Convert all_proteins to a list\n",
    "all_proteins = list(all_proteins)\n",
    "\n",
    "# Second Pass: Process data and write to output\n",
    "print(\"Second Pass: Filtering dataset and writing to new Parquet file...\")\n",
    "\n",
    "# Initialize Parquet writer\n",
    "writer = None\n",
    "\n",
    "for rg in tqdm(range(total_row_groups), desc=\"Processing Batches\"):\n",
    "    # Read the row group\n",
    "    batch = pf.read_row_group(rg)\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Filter molecules that have at least one binds == 1\n",
    "    df = df[df['molecule_smiles'].isin(molecule_binds.keys())]\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Prepare to select rows to include\n",
    "        rows_to_include = []\n",
    "        \n",
    "        # Process each molecule in the batch\n",
    "        for molecule, group in df.groupby('molecule_smiles'):\n",
    "            binds_1_proteins = molecule_binds[molecule]\n",
    "            num_binds_1 = len(binds_1_proteins)\n",
    "            \n",
    "            if num_binds_1 > 1:\n",
    "                # Include all rows for this molecule\n",
    "                rows_to_include.append(group)\n",
    "            elif num_binds_1 == 1:\n",
    "                # Include the positive binding row\n",
    "                positive_row = group[group['binds'] == 1]\n",
    "                \n",
    "                # Include one negative binding row\n",
    "                unbound_proteins = set(all_proteins) - binds_1_proteins\n",
    "                # Select one unbound protein\n",
    "                unbound_protein = unbound_proteins.pop()\n",
    "                negative_row = group[(group['protein_name'] == unbound_protein) & (group['binds'] == 0)]\n",
    "                \n",
    "                # If negative_row is empty, skp it\n",
    "                if  not negative_row.empty:\n",
    "                    # Append the positive and negative rows\n",
    "                    rows_to_include.append(pd.concat([positive_row, negative_row]))\n",
    "\n",
    "        \n",
    "        if rows_to_include:\n",
    "            filtered_df = pd.concat(rows_to_include)\n",
    "            \n",
    "            # Convert to PyArrow Table\n",
    "            table = pa.Table.from_pandas(filtered_df)\n",
    "            \n",
    "            # Initialize the Parquet writer if not already done\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(output_file, table.schema)\n",
    "            \n",
    "            # Write the table to the Parquet file\n",
    "            writer.write_table(table)\n",
    "            \n",
    "            # Clear variables to free memory\n",
    "            del table, filtered_df\n",
    "    \n",
    "    # Clear variables to free memory\n",
    "    del df, batch\n",
    "\n",
    "# Close the Parquet writer\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "print(\"Filtering completed. Filtered dataset saved to 'filtered_train.parquet'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978c650-5392-4775-80c8-cf190519c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('filtered_train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f998fa-441c-4f0d-8d81-158d50b3831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, DataLoader, Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e2308-dc4e-49ad-b3db-a0e5cdfe5d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Assuming the dataset is in a Parquet file named 'filtered_train.parquet'\n",
    "df = pd.read_parquet('filtered_train.parquet')\n",
    "\n",
    "# Check the distribution of 'binds'\n",
    "print(\"Distribution of 'binds' in the dataset:\")\n",
    "print(df['binds'].value_counts())\n",
    "\n",
    "# Stratified splitting\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['binds'],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f3ad4c8-711f-49cb-a28d-a5a9ce2d7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, HeteroData, DataLoader\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.Polypeptide import three_to_index, index_to_one\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set random seed and device\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba3d7b1-2dbc-475b-8976-ced84be1d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, Linear, global_mean_pool\n",
    "from torch.utils.data import random_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.Polypeptide import three_to_index, index_to_one\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Helper functions\n",
    "def residue_name_to_idx(res_name_one):\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G',\n",
    "                   'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S',\n",
    "                   'T', 'W', 'Y', 'V']\n",
    "    if res_name_one in amino_acids:\n",
    "        return amino_acids.index(res_name_one)\n",
    "    else:\n",
    "        return len(amino_acids)  # Unknown amino acid\n",
    "\n",
    "def process_protein(pdb_file, threshold=5.0):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('protein', pdb_file)\n",
    "\n",
    "    amino_acids = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if is_aa(residue):\n",
    "                    amino_acids.append(residue)\n",
    "\n",
    "    amino_acid_types = [index_to_one(three_to_index(residue.get_resname())) for residue in amino_acids]\n",
    "    unique_amino_acids = list(set(amino_acid_types))\n",
    "\n",
    "    data = HeteroData()\n",
    "\n",
    "    node_features = {}\n",
    "    node_positions = {}\n",
    "    node_counter = 0\n",
    "\n",
    "    # Initialize node features and positions\n",
    "    for aa_type in unique_amino_acids:\n",
    "        node_features[aa_type] = []\n",
    "        node_positions[aa_type] = []\n",
    "\n",
    "    for idx, (residue, aa_type) in enumerate(zip(amino_acids, amino_acid_types)):\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            pos = ca_atom.get_coord()\n",
    "        except KeyError:\n",
    "            pos = [0.0, 0.0, 0.0]\n",
    "        node_features[aa_type].append([residue_name_to_idx(aa_type)])\n",
    "        node_positions[aa_type].append(pos)\n",
    "        node_counter += 1\n",
    "\n",
    "    for aa_type in unique_amino_acids:\n",
    "        data[aa_type].x = torch.tensor(node_features[aa_type], dtype=torch.float)\n",
    "        data[aa_type].pos = torch.tensor(np.array(node_positions[aa_type]), dtype=torch.float)\n",
    "\n",
    "    # Build edges based on proximity\n",
    "    contact_edge_index = {}\n",
    "    edge_types = set()\n",
    "\n",
    "    # Mapping from global index to local index within node type\n",
    "    global_to_local_idx = {}\n",
    "    current_idx = {aa_type: 0 for aa_type in unique_amino_acids}\n",
    "\n",
    "    for aa_type in amino_acid_types:\n",
    "        global_to_local_idx[aa_type] = {}\n",
    "\n",
    "    for idx, aa_type in enumerate(amino_acid_types):\n",
    "        global_idx = idx\n",
    "        local_idx = current_idx[aa_type]\n",
    "        global_to_local_idx[aa_type][global_idx] = local_idx\n",
    "        current_idx[aa_type] += 1\n",
    "\n",
    "    num_residues = len(amino_acids)\n",
    "    for i in range(num_residues):\n",
    "        residue_i = amino_acids[i]\n",
    "        aa_i = amino_acid_types[i]\n",
    "        try:\n",
    "            ca_i = residue_i['CA']\n",
    "            pos_i = ca_i.get_coord()\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j in range(i + 1, num_residues):  # Ensure j > i to avoid duplicates\n",
    "            residue_j = amino_acids[j]\n",
    "            aa_j = amino_acid_types[j]\n",
    "            try:\n",
    "                ca_j = residue_j['CA']\n",
    "                pos_j = ca_j.get_coord()\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            distance = np.linalg.norm(pos_i - pos_j)\n",
    "            if distance <= threshold:\n",
    "                # Define edge type in consistent order\n",
    "                if aa_i <= aa_j:\n",
    "                    edge_type = (aa_i, 'contact', aa_j)\n",
    "                    src_aa, tgt_aa = aa_i, aa_j\n",
    "                    src_global, tgt_global = i, j\n",
    "                else:\n",
    "                    edge_type = (aa_j, 'contact', aa_i)\n",
    "                    src_aa, tgt_aa = aa_j, aa_i\n",
    "                    src_global, tgt_global = j, i\n",
    "\n",
    "                # Initialize edge list if not present\n",
    "                if edge_type not in contact_edge_index:\n",
    "                    contact_edge_index[edge_type] = []\n",
    "\n",
    "                # Get local indices within their respective node types\n",
    "                src_local = global_to_local_idx[src_aa][src_global]\n",
    "                tgt_local = global_to_local_idx[tgt_aa][tgt_global]\n",
    "\n",
    "                # Append edge\n",
    "                contact_edge_index[edge_type].append([src_local, tgt_local])\n",
    "                edge_types.add(edge_type)\n",
    "\n",
    "    # Assign edges to HeteroData\n",
    "    for edge_type, edges in contact_edge_index.items():\n",
    "        if len(edges) > 0:\n",
    "            # Since the graph is undirected, add reverse edges\n",
    "            reverse_edges = [[tgt, src] for src, tgt in edges]\n",
    "            all_edges = edges + reverse_edges\n",
    "            edge_tensor = torch.tensor(all_edges, dtype=torch.long).t().contiguous()\n",
    "            data[edge_type].edge_index = edge_tensor\n",
    "\n",
    "    data.node_types = set(unique_amino_acids)\n",
    "    data.edge_types = edge_types\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19cd793-2aef-4bee-a847-e385e0387d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, pre_transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        super(MoleculeDataset, self).__init__(None, transform, pre_transform)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def get(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        smiles = row['molecule_smiles']\n",
    "        binds = row['binds']\n",
    "        protein_name = row['protein_name']\n",
    "\n",
    "        # Convert SMILES to molecular graph\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None  # Skip invalid SMILES\n",
    "\n",
    "        mol = Chem.AddHs(mol)\n",
    "        AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "\n",
    "        atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        unique_atom_types = list(set(atom_types))\n",
    "\n",
    "        data = HeteroData()\n",
    "\n",
    "        atom_type_to_indices = {atype: [] for atype in unique_atom_types}\n",
    "        atom_features = []\n",
    "        atom_positions = []\n",
    "        for i, atom in enumerate(mol.GetAtoms()):\n",
    "            atype = atom.GetSymbol()\n",
    "            atom_type_to_indices[atype].append(i)\n",
    "            atom_features.append(self.get_atom_features(atom))\n",
    "            pos = mol.GetConformer().GetAtomPosition(i)\n",
    "            atom_positions.append(np.array([pos.x, pos.y, pos.z], dtype=np.float32))\n",
    "\n",
    "        # Assign node features and positions per atom type\n",
    "        for atype in unique_atom_types:\n",
    "            idx = atom_type_to_indices[atype]\n",
    "            x = torch.tensor([atom_features[i] for i in idx], dtype=torch.float)\n",
    "            pos = torch.tensor(np.array([atom_positions[i] for i in idx]), dtype=torch.float)\n",
    "            data[atype].x = x\n",
    "            data[atype].pos = pos\n",
    "\n",
    "        # Precompute mapping from global atom index to local index within atom type\n",
    "        atom_type_to_local_idx = {\n",
    "            atype: {global_idx: local_idx for local_idx, global_idx in enumerate(idxs)}\n",
    "            for atype, idxs in atom_type_to_indices.items()\n",
    "        }\n",
    "\n",
    "        # Assign bond edges to specific edge types based on atom types\n",
    "        bond_edges = {}\n",
    "        edge_types = set()\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            atype_i = atom_types[i]\n",
    "            atype_j = atom_types[j]\n",
    "            \n",
    "            # Define edge type in consistent order\n",
    "            if atype_i <= atype_j:\n",
    "                edge_type = (atype_i, 'bond', atype_j)\n",
    "                src_atype, tgt_atype = atype_i, atype_j\n",
    "                src_idx, tgt_idx = i, j\n",
    "            else:\n",
    "                edge_type = (atype_j, 'bond', atype_i)\n",
    "                src_atype, tgt_atype = atype_j, atype_i\n",
    "                src_idx, tgt_idx = j, i\n",
    "            edge_types.add(edge_type)\n",
    "            if edge_type not in bond_edges:\n",
    "                bond_edges[edge_type] = {'edge_index': [], 'edge_attr': []}\n",
    "\n",
    "            # Retrieve local indices using precomputed mapping\n",
    "            src_local = atom_type_to_local_idx[src_atype][src_idx]\n",
    "            tgt_local = atom_type_to_local_idx[tgt_atype][tgt_idx]\n",
    "\n",
    "            # Append both directions for undirected bonds\n",
    "            bond_edges[edge_type]['edge_index'].append([src_local, tgt_local])\n",
    "            bond_edges[edge_type]['edge_index'].append([tgt_local, src_local])\n",
    "\n",
    "            # Append bond features for both directions\n",
    "            bond_feature = self.get_bond_features(bond)\n",
    "            bond_edges[edge_type]['edge_attr'].append(bond_feature)\n",
    "            bond_edges[edge_type]['edge_attr'].append(bond_feature)\n",
    "\n",
    "        # Assign bond edges to HeteroData\n",
    "        for edge_type, attrs in bond_edges.items():\n",
    "            edge_index = torch.tensor(attrs['edge_index'], dtype=torch.long).t().contiguous()\n",
    "            edge_attr = torch.tensor(attrs['edge_attr'], dtype=torch.float)\n",
    "            data[edge_type].edge_index = edge_index\n",
    "            data[edge_type].edge_attr = edge_attr\n",
    "\n",
    "        # Add binding label and metadata\n",
    "        data['smolecule'].y = torch.tensor([binds], dtype=torch.float)\n",
    "        data['smolecule'].smiles = smiles\n",
    "        data['smolecule'].protein_name = protein_name\n",
    "\n",
    "        data.node_types = set(unique_atom_types)\n",
    "        data.edge_types = set(bond_edges.keys())\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def get_atom_features(atom):\n",
    "        return [\n",
    "            atom.GetAtomicNum(),\n",
    "            atom.GetTotalDegree(),\n",
    "            atom.GetFormalCharge(),\n",
    "            atom.GetHybridization().real,\n",
    "            int(atom.GetIsAromatic())\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bond_features(bond):\n",
    "        bond_type = bond.GetBondType()\n",
    "        bond_dict = {\n",
    "            Chem.rdchem.BondType.SINGLE: 0,\n",
    "            Chem.rdchem.BondType.DOUBLE: 1,\n",
    "            Chem.rdchem.BondType.TRIPLE: 2,\n",
    "            Chem.rdchem.BondType.AROMATIC: 3\n",
    "        }\n",
    "\n",
    "        return [\n",
    "            bond_dict.get(bond_type, -1)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ed5480-bc14-4b3f-8383-ff1760e98459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, dataframe, protein_graphs, transform=None, pre_transform=None, cache_dir='./processed'):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.protein_graphs = protein_graphs\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        super(CombinedDataset, self).__init__(None, transform, pre_transform)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def get(self, idx):\n",
    "        processed_file = os.path.join(self.cache_dir, f'data_{idx}.pt')\n",
    "        if os.path.exists(processed_file):\n",
    "            molecule_data, protein_data = torch.load(processed_file)\n",
    "        else:\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            smiles = row['molecule_smiles']\n",
    "            binds = row['binds']\n",
    "            protein_name = row['protein_name']\n",
    "\n",
    "            mol_dataset = MoleculeDataset(pd.DataFrame([row]))\n",
    "            molecule_data = mol_dataset.get(0)\n",
    "            if molecule_data is None:\n",
    "                return None\n",
    "\n",
    "            protein_data = self.protein_graphs.get(protein_name, HeteroData())\n",
    "\n",
    "            molecule_data.y = torch.tensor([binds], dtype=torch.float)\n",
    "            molecule_data.smiles = smiles\n",
    "            molecule_data.protein_name = protein_name\n",
    "\n",
    "            torch.save((molecule_data, protein_data), processed_file)\n",
    "\n",
    "        return molecule_data, protein_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de232ea6-bc50-431f-80b6-7039be7b0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def collect_molecule_node_and_edge_types(df):\n",
    "    molecule_node_types = set()\n",
    "    molecule_edge_types = set()\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting molecule types\"):\n",
    "        smiles = row['molecule_smiles']\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        mol = Chem.AddHs(mol)\n",
    "        atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        molecule_node_types.update(atom_types)\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            atype_i = atom_types[i]\n",
    "            atype_j = atom_types[j]\n",
    "            if atype_i <= atype_j:\n",
    "                edge_type = (atype_i, 'bond', atype_j)\n",
    "            else:\n",
    "                edge_type = (atype_j, 'bond', atype_i)\n",
    "            molecule_edge_types.add(edge_type)\n",
    "    return sorted(molecule_node_types), sorted(molecule_edge_types)\n",
    "\n",
    "def collect_protein_node_and_edge_types(protein_graphs):\n",
    "    protein_node_types = set()\n",
    "    protein_edge_types = set()\n",
    "    for protein_data in protein_graphs.values():\n",
    "        protein_node_types.update(protein_data.node_types)\n",
    "        protein_edge_types.update(protein_data.edge_types)\n",
    "    return sorted(protein_node_types), sorted(protein_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dfd96d2-231d-4b13-a6ca-6243fd5c2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=4):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_K = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_V = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.scale = self.head_dim ** 0.5\n",
    "\n",
    "    def forward(self, query_nodes, key_nodes):\n",
    "        # query_nodes: [N_q, hidden_dim]\n",
    "        # key_nodes: [N_k, hidden_dim]\n",
    "\n",
    "        N_q = query_nodes.size(0)\n",
    "        N_k = key_nodes.size(0)\n",
    "\n",
    "        Q = self.W_Q(query_nodes)  # [N_q, hidden_dim]\n",
    "        K = self.W_K(key_nodes)    # [N_k, hidden_dim]\n",
    "        V = self.W_V(key_nodes)    # [N_k, hidden_dim]\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(N_q, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_q, head_dim]\n",
    "        K = K.view(N_k, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_k, head_dim]\n",
    "        V = V.view(N_k, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_k, head_dim]\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [num_heads, N_q, N_k]\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)                # [num_heads, N_q, N_k]\n",
    "\n",
    "        # Compute attended values\n",
    "        out = torch.matmul(attn_weights, V)  # [num_heads, N_q, head_dim]\n",
    "        out = out.permute(1, 0, 2).contiguous().view(N_q, -1)  # [N_q, hidden_dim]\n",
    "\n",
    "        return out\n",
    "\n",
    "class CrossGraphAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_attention_heads=4):\n",
    "        super(CrossGraphAttentionModel, self).__init__()\n",
    "\n",
    "        # Molecule GNN Encoder\n",
    "        self.mol_conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in molecule_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.mol_conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in molecule_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Protein GNN Encoder\n",
    "        self.prot_conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in protein_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.prot_conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in protein_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Cross-Attention Layers\n",
    "        self.cross_attn_mol_to_prot = CrossAttentionLayer(hidden_dim, num_attention_heads)\n",
    "        self.cross_attn_prot_to_mol = CrossAttentionLayer(hidden_dim, num_attention_heads)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, mol_data, prot_data):\n",
    "        # Molecule GNN Encoding\n",
    "        x_mol_dict = mol_data.x_dict\n",
    "        edge_index_mol_dict = mol_data.edge_index_dict\n",
    "\n",
    "        x_mol_dict = self.mol_conv1(x_mol_dict, edge_index_mol_dict)\n",
    "        x_mol_dict = {key: F.relu(x) for key, x in x_mol_dict.items()}\n",
    "\n",
    "        x_mol_dict = self.mol_conv2(x_mol_dict, edge_index_mol_dict)\n",
    "        x_mol_dict = {key: F.relu(x) for key, x in x_mol_dict.items()}\n",
    "\n",
    "        # Concatenate molecule node embeddings\n",
    "        mol_node_embeddings = []\n",
    "        for nt in molecule_node_types:\n",
    "            if nt in x_mol_dict:\n",
    "                mol_node_embeddings.append(x_mol_dict[nt])\n",
    "        H_mol = torch.cat(mol_node_embeddings, dim=0)\n",
    "\n",
    "        # Protein GNN Encoding\n",
    "        x_prot_dict = prot_data.x_dict\n",
    "        edge_index_prot_dict = prot_data.edge_index_dict\n",
    "\n",
    "        x_prot_dict = self.prot_conv1(x_prot_dict, edge_index_prot_dict)\n",
    "        x_prot_dict = {key: F.relu(x) for key, x in x_prot_dict.items()}\n",
    "\n",
    "        x_prot_dict = self.prot_conv2(x_prot_dict, edge_index_prot_dict)\n",
    "        x_prot_dict = {key: F.relu(x) for key, x in x_prot_dict.items()}\n",
    "\n",
    "        # Concatenate protein node embeddings\n",
    "        prot_node_embeddings = []\n",
    "        for nt in protein_node_types:\n",
    "            if nt in x_prot_dict:\n",
    "                prot_node_embeddings.append(x_prot_dict[nt])\n",
    "        H_prot = torch.cat(prot_node_embeddings, dim=0)\n",
    "\n",
    "        # Cross-Attention\n",
    "        H_mol_attn = self.cross_attn_mol_to_prot(H_mol, H_prot)\n",
    "        H_prot_attn = self.cross_attn_prot_to_mol(H_prot, H_mol)\n",
    "\n",
    "        # Combine original and attended embeddings\n",
    "        H_mol_combined = H_mol + H_mol_attn\n",
    "        H_prot_combined = H_prot + H_prot_attn\n",
    "\n",
    "        # Global Pooling\n",
    "        mol_batch = mol_data.batch if hasattr(mol_data, 'batch') else torch.zeros(H_mol_combined.size(0), dtype=torch.long, device=H_mol_combined.device)\n",
    "        prot_batch = prot_data.batch if hasattr(prot_data, 'batch') else torch.zeros(H_prot_combined.size(0), dtype=torch.long, device=H_prot_combined.device)\n",
    "\n",
    "        z_mol = global_mean_pool(H_mol_combined, mol_batch)\n",
    "        z_prot = global_mean_pool(H_prot_combined, prot_batch)\n",
    "\n",
    "        # Joint Representation\n",
    "        z_joint = torch.cat([z_mol, z_prot], dim=1)\n",
    "\n",
    "        # Prediction\n",
    "        x = F.relu(self.fc1(z_joint))\n",
    "        out = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51caf7-161d-48a6-82f7-0b7e75117802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split completed.\n",
      "Training set size: 2169734\n",
      "Validation set size: 464943\n",
      "Test set size: 464943\n",
      "Collecting molecule node and edge types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting molecule types:   0%|        | 796/3099620 [00:00<18:54, 2730.99it/s]"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_parquet('filtered_train.parquet')\n",
    "\n",
    "# Stratified splitting\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nSplit completed.\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Process and store protein graphs\n",
    "protein_graphs = {}\n",
    "protein_pdb_files = {\n",
    "    'BRD4': './BRD4.pdb',\n",
    "    'HSA': './ALB.pdb',\n",
    "    'sEH': './EPH.pdb'\n",
    "}\n",
    "\n",
    "for protein_name, pdb_file in protein_pdb_files.items():\n",
    "    if os.path.exists(pdb_file):\n",
    "        protein_data = process_protein(pdb_file)\n",
    "        protein_graphs[protein_name] = protein_data\n",
    "    else:\n",
    "        print(f\"PDB file {pdb_file} for {protein_name} does not exist.\")\n",
    "\n",
    "# Collect node and edge types with progress tracking\n",
    "print(\"Collecting molecule node and edge types...\")\n",
    "molecule_node_types, molecule_edge_types = collect_molecule_node_and_edge_types(df)\n",
    "\n",
    "print(\"Collecting protein node and edge types...\")\n",
    "protein_node_types, protein_edge_types = collect_protein_node_and_edge_types(protein_graphs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6327f-d5cd-4fed-9471-f6db72306c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import CombinedDataset, MoleculeDataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CombinedDataset(train_df, protein_graphs)\n",
    "val_dataset = CombinedDataset(val_df, protein_graphs)\n",
    "test_dataset = CombinedDataset(test_df, protein_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99aeef-8131-49da-9d14-2c780ee6e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    mol_batch = [item[0] for item in batch if item is not None and item[0] is not None]\n",
    "    prot_batch = [item[1] for item in batch if item is not None and item[0] is not None]\n",
    "\n",
    "    mol_batch = Batch.from_data_list(mol_batch)\n",
    "    prot_batch = Batch.from_data_list(prot_batch)\n",
    "\n",
    "    return mol_batch, prot_batch\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = GeoDataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "val_loader = GeoDataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "test_loader = GeoDataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5baf606-8eea-4ab1-9b47-cab4b60eb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "hidden_dim = 64\n",
    "num_attention_heads = 4\n",
    "\n",
    "model = CrossGraphAttentionModel(\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_attention_heads=num_attention_heads\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mol_data, prot_data in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        mol_data = mol_data.to(device)\n",
    "        prot_data = prot_data.to(device)\n",
    "        out = model(mol_data, prot_data)\n",
    "        y = mol_data.y.to(device)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(loader, mode='Validation'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mol_data, prot_data in tqdm(loader, desc=mode):\n",
    "            mol_data = mol_data.to(device)\n",
    "            prot_data = prot_data.to(device)\n",
    "            out = model(mol_data, prot_data)\n",
    "            y = mol_data.y.to(device)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss = evaluate(val_loader, mode='Validation')\n",
    "    print(f'Epoch: {epoch:02d}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'cross_graph_attention_model.pth')\n",
    "\n",
    "# Prediction and evaluation on test data\n",
    "def predict(loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for mol_data, prot_data in tqdm(loader, desc=\"Testing\"):\n",
    "            mol_data = mol_data.to(device)\n",
    "            prot_data = prot_data.to(device)\n",
    "            out = model(mol_data, prot_data)\n",
    "            predictions.extend(out.cpu().numpy())\n",
    "            true_labels.extend(mol_data.y.cpu().numpy())\n",
    "    return predictions, true_labels\n",
    "\n",
    "test_predictions, test_true = predict(test_loader)\n",
    "\n",
    "# Apply a threshold to obtain binary predictions\n",
    "threshold = 0.5\n",
    "test_pred_binary = [1 if p >= threshold else 0 for p in test_predictions]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(test_true, test_pred_binary)\n",
    "roc_auc = roc_auc_score(test_true, test_predictions)\n",
    "precision = precision_score(test_true, test_pred_binary)\n",
    "recall = recall_score(test_true, test_pred_binary)\n",
    "f1 = f1_score(test_true, test_pred_binary)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96af9b-fc09-467d-9239-89a793fc87d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
