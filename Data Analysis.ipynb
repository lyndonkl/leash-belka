{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b952cb-00a1-4e51-bbd5-02fcc0baaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('train.parquet')\n",
    "print(parquet_file.schema.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5e258-03e7-4745-b09d-783c1ed20fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e723c9-0aed-4669-89a4-a1f45bcf0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to input and output Parquet files\n",
    "input_file = 'train.parquet'\n",
    "output_file = 'filtered_train.parquet'\n",
    "\n",
    "# Remove output file if it exists\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Open the Parquet file\n",
    "pf = pq.ParquetFile(input_file)\n",
    "\n",
    "# Get total number of row groups (batches)\n",
    "total_row_groups = pf.num_row_groups\n",
    "\n",
    "# First Pass: Build mapping of molecule_smiles to the set of proteins it binds to\n",
    "print(\"First Pass: Building molecule to proteins mapping...\")\n",
    "\n",
    "# Initialize a dictionary to store mappings\n",
    "molecule_binds = defaultdict(set)\n",
    "all_proteins = set()\n",
    "\n",
    "for rg in tqdm(range(total_row_groups), desc=\"Processing Batches\"):\n",
    "    # Read a row group with necessary columns\n",
    "    batch = pf.read_row_group(rg, columns=['molecule_smiles', 'protein_name', 'binds'])\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Update all_proteins set\n",
    "    all_proteins.update(df['protein_name'].unique())\n",
    "    \n",
    "    # Filter rows where 'binds' == 1\n",
    "    df_binds_1 = df[df['binds'] == 1]\n",
    "    \n",
    "    # Update molecule_binds mapping\n",
    "    for idx, row in df_binds_1.iterrows():\n",
    "        molecule = row['molecule_smiles']\n",
    "        protein = row['protein_name']\n",
    "        molecule_binds[molecule].add(protein)\n",
    "    \n",
    "    # Clear variables to free memory\n",
    "    del df, df_binds_1, batch\n",
    "\n",
    "# Convert all_proteins to a list\n",
    "all_proteins = list(all_proteins)\n",
    "\n",
    "# Second Pass: Process data and write to output\n",
    "print(\"Second Pass: Filtering dataset and writing to new Parquet file...\")\n",
    "\n",
    "# Initialize Parquet writer\n",
    "writer = None\n",
    "\n",
    "for rg in tqdm(range(total_row_groups), desc=\"Processing Batches\"):\n",
    "    # Read the row group\n",
    "    batch = pf.read_row_group(rg)\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Filter molecules that have at least one binds == 1\n",
    "    df = df[df['molecule_smiles'].isin(molecule_binds.keys())]\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Prepare to select rows to include\n",
    "        rows_to_include = []\n",
    "        \n",
    "        # Process each molecule in the batch\n",
    "        for molecule, group in df.groupby('molecule_smiles'):\n",
    "            binds_1_proteins = molecule_binds[molecule]\n",
    "            num_binds_1 = len(binds_1_proteins)\n",
    "            \n",
    "            if num_binds_1 > 1:\n",
    "                # Include all rows for this molecule\n",
    "                rows_to_include.append(group)\n",
    "            elif num_binds_1 == 1:\n",
    "                # Include the positive binding row\n",
    "                positive_row = group[group['binds'] == 1]\n",
    "                \n",
    "                # Include one negative binding row\n",
    "                unbound_proteins = set(all_proteins) - binds_1_proteins\n",
    "                # Select one unbound protein\n",
    "                unbound_protein = unbound_proteins.pop()\n",
    "                negative_row = group[(group['protein_name'] == unbound_protein) & (group['binds'] == 0)]\n",
    "                \n",
    "                # If negative_row is empty, skp it\n",
    "                if  not negative_row.empty:\n",
    "                    # Append the positive and negative rows\n",
    "                    rows_to_include.append(pd.concat([positive_row, negative_row]))\n",
    "\n",
    "        \n",
    "        if rows_to_include:\n",
    "            filtered_df = pd.concat(rows_to_include)\n",
    "            \n",
    "            # Convert to PyArrow Table\n",
    "            table = pa.Table.from_pandas(filtered_df)\n",
    "            \n",
    "            # Initialize the Parquet writer if not already done\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(output_file, table.schema)\n",
    "            \n",
    "            # Write the table to the Parquet file\n",
    "            writer.write_table(table)\n",
    "            \n",
    "            # Clear variables to free memory\n",
    "            del table, filtered_df\n",
    "    \n",
    "    # Clear variables to free memory\n",
    "    del df, batch\n",
    "\n",
    "# Close the Parquet writer\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "print(\"Filtering completed. Filtered dataset saved to 'filtered_train.parquet'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978c650-5392-4775-80c8-cf190519c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('filtered_train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f998fa-441c-4f0d-8d81-158d50b3831e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x31b7426b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, DataLoader, Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441e2308-dc4e-49ad-b3db-a0e5cdfe5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of 'binds' in the dataset:\n",
      "binds\n",
      "1    1589903\n",
      "0    1509717\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Split completed.\n",
      "Training set size: 2169734\n",
      "Validation set size: 464943\n",
      "Test set size: 464943\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Assuming the dataset is in a Parquet file named 'filtered_train.parquet'\n",
    "df = pd.read_parquet('filtered_train.parquet')\n",
    "\n",
    "# Check the distribution of 'binds'\n",
    "print(\"Distribution of 'binds' in the dataset:\")\n",
    "print(df['binds'].value_counts())\n",
    "\n",
    "# Stratified splitting\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nSplit completed.\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f3ad4c8-711f-49cb-a28d-a5a9ce2d7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, HeteroData, DataLoader\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.Polypeptide import three_to_index, index_to_one\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set random seed and device\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba3d7b1-2dbc-475b-8976-ced84be1d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residue_name_to_idx(res_name_one):\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G',\n",
    "                   'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S',\n",
    "                   'T', 'W', 'Y', 'V']\n",
    "    if res_name_one in amino_acids:\n",
    "        return amino_acids.index(res_name_one)\n",
    "    else:\n",
    "        return len(amino_acids)  # Unknown amino acid\n",
    "\n",
    "def process_protein(pdb_file, threshold=5.0):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('protein', pdb_file)\n",
    "\n",
    "    amino_acids = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if is_aa(residue):\n",
    "                    amino_acids.append(residue)\n",
    "\n",
    "    amino_acid_types = [index_to_one(three_to_index(residue.get_resname())) for residue in amino_acids]\n",
    "    unique_amino_acids = list(set(amino_acid_types))\n",
    "\n",
    "    data = HeteroData()\n",
    "\n",
    "    node_features = {}\n",
    "    node_positions = {}\n",
    "    for aa_type in unique_amino_acids:\n",
    "        node_features[aa_type] = []\n",
    "        node_positions[aa_type] = []\n",
    "\n",
    "    for residue, aa_type in zip(amino_acids, amino_acid_types):\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            pos = ca_atom.get_coord()\n",
    "        except KeyError:\n",
    "            pos = [0.0, 0.0, 0.0]\n",
    "        node_features[aa_type].append([residue_name_to_idx(aa_type)])\n",
    "        node_positions[aa_type].append(pos)\n",
    "\n",
    "    for aa_type in unique_amino_acids:\n",
    "        data[aa_type].x = torch.tensor(node_features[aa_type], dtype=torch.float)\n",
    "        data[aa_type].pos = torch.tensor(node_positions[aa_type], dtype=torch.float)\n",
    "\n",
    "    # Build edges based on proximity\n",
    "    contact_edge_index = {}\n",
    "    for src_aa in unique_amino_acids:\n",
    "        for tgt_aa in unique_amino_acids:\n",
    "            contact_edge_index[(src_aa, 'contact', tgt_aa)] = []\n",
    "\n",
    "    for i, residue_i in enumerate(amino_acids):\n",
    "        try:\n",
    "            ca_i = residue_i['CA']\n",
    "            pos_i = ca_i.get_coord()\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j, residue_j in enumerate(amino_acids):\n",
    "            if i == j:\n",
    "                continue\n",
    "            try:\n",
    "                ca_j = residue_j['CA']\n",
    "                pos_j = ca_j.get_coord()\n",
    "            except KeyError:\n",
    "                continue\n",
    "            distance = np.linalg.norm(pos_i - pos_j)\n",
    "            if distance <= threshold:\n",
    "                aa_i = amino_acid_types[i]\n",
    "                aa_j = amino_acid_types[j]\n",
    "                contact_edge_index[(aa_i, 'contact', aa_j)].append([i, j])\n",
    "\n",
    "    for edge_type, edges in contact_edge_index.items():\n",
    "        if len(edges) > 0:\n",
    "            edge_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            data[edge_type].edge_index = edge_tensor\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8e09cf-95a2-4f41-994c-9a35ed161e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataframe, protein_graphs, transform=None, pre_transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.protein_graphs = protein_graphs\n",
    "        super(MoleculeDataset, self).__init__(None, transform, pre_transform)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def get(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        smiles = row['molecule_smiles']\n",
    "        binds = row['binds']\n",
    "        protein_name = row['protein_name']\n",
    "\n",
    "        # Convert SMILES to molecular graph\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None  # Skip invalid SMILES\n",
    "\n",
    "        mol = Chem.AddHs(mol)\n",
    "        AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "\n",
    "        atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        unique_atom_types = list(set(atom_types))\n",
    "\n",
    "        data = HeteroData()\n",
    "\n",
    "        atom_type_to_indices = {atype: [] for atype in unique_atom_types}\n",
    "        atom_features = []\n",
    "        atom_positions = []\n",
    "        for i, atom in enumerate(mol.GetAtoms()):\n",
    "            atype = atom.GetSymbol()\n",
    "            atom_type_to_indices[atype].append(i)\n",
    "            atom_features.append(self.get_atom_features(atom))\n",
    "            pos = mol.GetConformer().GetAtomPosition(i)\n",
    "            atom_positions.append([pos.x, pos.y, pos.z])\n",
    "\n",
    "        # Assign node features and positions per atom type\n",
    "        for atype in unique_atom_types:\n",
    "            idx = atom_type_to_indices[atype]\n",
    "            x = torch.tensor([atom_features[i] for i in idx], dtype=torch.float)\n",
    "            pos = torch.tensor([atom_positions[i] for i in idx], dtype=torch.float)\n",
    "            data[atype].x = x\n",
    "            data[atype].pos = pos\n",
    "\n",
    "        # Precompute mapping from global atom index to local index within atom type\n",
    "        atom_type_to_local_idx = {\n",
    "            atype: {global_idx: local_idx for local_idx, global_idx in enumerate(idxs)}\n",
    "            for atype, idxs in atom_type_to_indices.items()\n",
    "        }\n",
    "\n",
    "        # Assign bond edges to specific edge types based on atom types\n",
    "        bond_edges = {}\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            atype_i = atom_types[i]\n",
    "            atype_j = atom_types[j]\n",
    "            edge_type = (atype_i, 'bond', atype_j)\n",
    "            \n",
    "            if edge_type not in bond_edges:\n",
    "                bond_edges[edge_type] = {'edge_index': [], 'edge_attr': []}\n",
    "            \n",
    "            # Retrieve local indices using precomputed mapping\n",
    "            src_local = atom_type_to_local_idx[atype_i][i]\n",
    "            tgt_local = atom_type_to_local_idx[atype_j][j]\n",
    "            \n",
    "            # Append both directions for undirected bonds\n",
    "            bond_edges[edge_type]['edge_index'].append([src_local, tgt_local])\n",
    "            bond_edges[edge_type]['edge_index'].append([tgt_local, src_local])\n",
    "            \n",
    "            # Append bond features for both directions\n",
    "            bond_feature = self.get_bond_features(bond)\n",
    "            bond_edges[edge_type]['edge_attr'].append(bond_feature)\n",
    "            bond_edges[edge_type]['edge_attr'].append(bond_feature)\n",
    "\n",
    "        # Assign bond edges to HeteroData\n",
    "        for edge_type, attrs in bond_edges.items():\n",
    "            edge_index = torch.tensor(attrs['edge_index'], dtype=torch.long).t().contiguous()\n",
    "            edge_attr = torch.tensor(attrs['edge_attr'], dtype=torch.float)\n",
    "            data[edge_type].edge_index = edge_index\n",
    "            data[edge_type].edge_attr = edge_attr\n",
    "\n",
    "        # Add binding label and metadata\n",
    "        data['smolecule'].y = torch.tensor([binds], dtype=torch.float)\n",
    "        data['smolecule'].smiles = smiles\n",
    "        data['smolecule'].protein_name = protein_name\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def get_atom_features(atom):\n",
    "        return [\n",
    "            atom.GetAtomicNum(),\n",
    "            atom.GetTotalDegree(),\n",
    "            atom.GetFormalCharge(),\n",
    "            atom.GetHybridization().real,\n",
    "            int(atom.GetIsAromatic())\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bond_features(bond):\n",
    "        bond_type = bond.GetBondType()\n",
    "        bond_dict = {\n",
    "            Chem.rdchem.BondType.SINGLE: 0,\n",
    "            Chem.rdchem.BondType.DOUBLE: 1,\n",
    "            Chem.rdchem.BondType.TRIPLE: 2,\n",
    "            Chem.rdchem.BondType.AROMATIC: 3\n",
    "        }\n",
    "\n",
    "        # Additional features\n",
    "        bond_length = bond.GetBondLength()\n",
    "\n",
    "        return [\n",
    "            bond_dict.get(bond_type, -1),\n",
    "            bond_length\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ed5480-bc14-4b3f-8383-ff1760e98459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, dataframe, protein_graphs, transform=None, pre_transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.protein_graphs = protein_graphs\n",
    "        super(CombinedDataset, self).__init__(None, transform, pre_transform)\n",
    "\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        smiles = row['molecule_smiles']\n",
    "        binds = row['binds']\n",
    "        protein_name = row['protein_name']\n",
    "        \n",
    "        # Process molecule\n",
    "        mol_dataset = MoleculeDataset(pd.DataFrame([row]), self.protein_graphs)\n",
    "        molecule_data = mol_dataset.get(0)\n",
    "        \n",
    "        # Process protein\n",
    "        protein_data = self.protein_graphs.get(protein_name, HeteroData())\n",
    "        \n",
    "        # Combine molecule and protein data\n",
    "        data = HeteroData()\n",
    "        \n",
    "        # Add molecule node types\n",
    "        for node_type in molecule_data.node_types:\n",
    "            data[node_type].x = molecule_data[node_type].x\n",
    "            data[node_type].pos = molecule_data[node_type].pos\n",
    "        \n",
    "        # Add molecule bond edges\n",
    "        for edge_type in molecule_data.edge_types:\n",
    "            data[edge_type].edge_index = molecule_data[edge_type].edge_index\n",
    "            data[edge_type].edge_attr = molecule_data[edge_type].edge_attr\n",
    "        \n",
    "        # Add protein node types\n",
    "        for node_type in protein_data.node_types:\n",
    "            if node_type in data.node_types:\n",
    "                data[node_type].x = torch.cat([data[node_type].x, protein_data[node_type].x], dim=0)\n",
    "                data[node_type].pos = torch.cat([data[node_type].pos, protein_data[node_type].pos], dim=0)\n",
    "            else:\n",
    "                data[node_type].x = protein_data[node_type].x\n",
    "                data[node_type].pos = protein_data[node_type].pos\n",
    "        \n",
    "        # Add protein contact edges\n",
    "        for edge_type in protein_data.edge_types:\n",
    "            data[edge_type].edge_index = protein_data[edge_type].edge_index\n",
    "        \n",
    "        # Add binding label and metadata\n",
    "        data['smolecule'].y = torch.tensor([binds], dtype=torch.float)\n",
    "        data['smolecule'].smiles = smiles\n",
    "        data['smolecule'].protein_name = protein_name\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de232ea6-bc50-431f-80b6-7039be7b0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_molecule_node_and_edge_types(df):\n",
    "    molecule_node_types = set()\n",
    "    molecule_edge_types = set()\n",
    "    for idx, row in df.iterrows():\n",
    "        smiles = row['molecule_smiles']\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        mol = Chem.AddHs(mol)\n",
    "        atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        molecule_node_types.update(atom_types)\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            atype_i = atom_types[i]\n",
    "            atype_j = atom_types[j]\n",
    "            edge_type = (atype_i, 'bond', atype_j)\n",
    "            molecule_edge_types.add(edge_type)\n",
    "    return sorted(molecule_node_types), sorted(molecule_edge_types)\n",
    "\n",
    "def collect_protein_node_and_edge_types(protein_graphs):\n",
    "    protein_node_types = set()\n",
    "    protein_edge_types = set()\n",
    "    for protein_data in protein_graphs.values():\n",
    "        protein_node_types.update(protein_data.node_types)\n",
    "        protein_edge_types.update(protein_data.edge_types)\n",
    "    return sorted(protein_node_types), sorted(protein_edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd96d2-231d-4b13-a6ca-6243fd5c2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, Linear, global_mean_pool\n",
    "\n",
    "class CrossGraphAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, molecule_node_types, protein_node_types, edge_types, hidden_dim=64):\n",
    "        super(CrossGraphAttentionModel, self).__init__()\n",
    "\n",
    "        self.molecule_node_types = molecule_node_types\n",
    "        self.protein_node_types = protein_node_types\n",
    "        self.edge_types = edge_types\n",
    "\n",
    "        # Create HeteroConv layers\n",
    "        self.conv1 = HeteroConv({\n",
    "            edge_type: GCNConv((-1, -1), hidden_dim)\n",
    "            for edge_type in self.edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            edge_type: GCNConv((-1, -1), hidden_dim)\n",
    "            for edge_type in self.edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Classification layers\n",
    "        self.fc1 = Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_dict = data.x_dict\n",
    "        edge_index_dict = data.edge_index_dict\n",
    "\n",
    "        # First convolution layer\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: torch.relu(x) for key, x in x_dict.items()}\n",
    "\n",
    "        # Second convolution layer\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        x_dict = {key: torch.relu(x) for key, x in x_dict.items()}\n",
    "\n",
    "        # Global pooling for molecule and protein nodes separately\n",
    "        mol_embeddings = []\n",
    "        for nt in self.molecule_node_types:\n",
    "            if nt in x_dict:\n",
    "                x = x_dict[nt]\n",
    "                batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "                mol_embeddings.append(global_mean_pool(x, batch))\n",
    "\n",
    "        if mol_embeddings:\n",
    "            mol_rep = torch.mean(torch.stack(mol_embeddings), dim=0)\n",
    "        else:\n",
    "            mol_rep = torch.zeros((1, self.fc1.in_features // 2), device=data['smolecule'].y.device)\n",
    "\n",
    "        prot_embeddings = []\n",
    "        for nt in self.protein_node_types:\n",
    "            if nt in x_dict:\n",
    "                x = x_dict[nt]\n",
    "                batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "                prot_embeddings.append(global_mean_pool(x, batch))\n",
    "\n",
    "        if prot_embeddings:\n",
    "            prot_rep = torch.mean(torch.stack(prot_embeddings), dim=0)\n",
    "        else:\n",
    "            prot_rep = torch.zeros((1, self.fc1.in_features // 2), device=data['smolecule'].y.device)\n",
    "\n",
    "        # Combine representations\n",
    "        combined = torch.cat([mol_rep, prot_rep], dim=1)\n",
    "\n",
    "        # Classification\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        out = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51caf7-161d-48a6-82f7-0b7e75117802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_parquet('filtered_train.parquet')\n",
    "\n",
    "# Stratified splitting\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Process and store protein graphs\n",
    "protein_graphs = {}\n",
    "protein_pdb_files = {\n",
    "    'BRD4': './BRD4.pdb',\n",
    "    'HSA': './ALB.pdb',\n",
    "    'sEH': './EPH.pdb'\n",
    "}\n",
    "\n",
    "for protein_name, pdb_file in protein_pdb_files.items():\n",
    "    if os.path.exists(pdb_file):\n",
    "        protein_data = process_protein(pdb_file)\n",
    "        protein_graphs[protein_name] = protein_data\n",
    "    else:\n",
    "        print(f\"PDB file {pdb_file} for {protein_name} does not exist.\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CombinedDataset(train_df, protein_graphs)\n",
    "val_dataset = CombinedDataset(val_df, protein_graphs)\n",
    "test_dataset = CombinedDataset(test_df, protein_graphs)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Instantiate the model\n",
    "hidden_dim = 64\n",
    "model = CrossGraphAttentionModel(hidden_dim=hidden_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        y = data['smolecule'].y\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(loader, mode='Validation'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc=mode):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            y = data['smolecule'].y\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss = evaluate(val_loader, mode='Validation')\n",
    "    print(f'Epoch: {epoch:02d}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Prediction on test data\n",
    "def predict(loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc=\"Testing\"):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            predictions.extend(out.cpu().numpy())\n",
    "            true_labels.extend(data['smolecule'].y.cpu().numpy())\n",
    "    return predictions, true_labels\n",
    "\n",
    "test_predictions, test_true = predict(test_loader)\n",
    "\n",
    "# Apply a threshold to obtain binary predictions\n",
    "threshold = 0.5\n",
    "test_pred_binary = [1 if p >= threshold else 0 for p in test_predictions]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(test_true, test_pred_binary)\n",
    "roc_auc = roc_auc_score(test_true, test_predictions)\n",
    "precision = precision_score(test_true, test_pred_binary)\n",
    "recall = recall_score(test_true, test_pred_binary)\n",
    "f1 = f1_score(test_true, test_pred_binary)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
